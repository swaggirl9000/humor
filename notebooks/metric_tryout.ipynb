{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/ada/humor/.venv/lib/python3.8/site-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /home/ada/humor/.venv/lib/python3.8/site-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: numpy in /home/ada/humor/.venv/lib/python3.8/site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ada/humor/.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ada/humor/.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ada/humor/.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ada/humor/.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ada/humor/.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/ada/humor/.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes) (70.1.0)\n",
      "Requirement already satisfied: wheel in /home/ada/humor/.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes) (0.43.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ada/humor/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import inseq\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv('/home/ada/humor/data/stand_up_dataset/standup_transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comedian</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony_Jeselnik</td>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anthony_Jeselnik_2</td>\n",
       "      <td>No one should ever ask me to speak at anyone’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ali_Wong</td>\n",
       "      <td>The last time I was at home in San Francisco, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ali_Wong_2</td>\n",
       "      <td>I need to have children to keep me company whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chelsea_Peretti</td>\n",
       "      <td>A friend of a friend just posted like 500 enga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             comedian                                         transcript\n",
       "0    Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...\n",
       "1  Anthony_Jeselnik_2   No one should ever ask me to speak at anyone’...\n",
       "2            Ali_Wong  The last time I was at home in San Francisco, ...\n",
       "3          Ali_Wong_2  I need to have children to keep me company whe...\n",
       "4     Chelsea_Peretti  A friend of a friend just posted like 500 enga..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def split_transcript(transcript):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(transcript.strip())\n",
    "    total_sentences = len(sentences)\n",
    "    sentences_per_part = total_sentences // 4\n",
    "\n",
    "    part1 = sentences[:sentences_per_part]\n",
    "    part2 = sentences[sentences_per_part:2*sentences_per_part]\n",
    "    part3 = sentences[2*sentences_per_part:3*sentences_per_part]\n",
    "    part4 = sentences[3*sentences_per_part:]\n",
    "\n",
    "    part1_text = ' '.join(part1)\n",
    "    part2_text = ' '.join(part2)\n",
    "    part3_text = ' '.join(part3)\n",
    "    part4_text = ' '.join(part4)\n",
    "\n",
    "    return [part1_text, part2_text, part3_text, part4_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_ali_wong = transcripts.loc[transcripts['comedian'] == 'Ali_Wong', 'transcript'].values[0]\n",
    "parts = split_transcript(transcript_ali_wong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory cost:  3890 Mb\n"
     ]
    }
   ],
   "source": [
    "Q8BIT = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = inseq.load_model(\n",
    "    model=\"google/gemma-2b-it\", \n",
    "    attribution_method=\"input_x_gradient\",\n",
    "    model_kwargs=dict(\n",
    "        device_map=\"cuda:0\",\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        quantization_config=Q8BIT,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"memory cost: \", model.model.get_memory_footprint() >> 20, \"Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"This is a stand-up comedy transcript: '''{parts[1]}'''\\nExtract the key humorous lines and punchlines:\"\n",
    "\n",
    "# out = model.attribute(\n",
    "# prompt,\n",
    "# n_steps=16,\n",
    "# batch_size=1,\n",
    "# generation_args=dict(\n",
    "#     max_new_tokens=65,\n",
    "# )\n",
    "# )\n",
    "# ()\n",
    "# out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.sequence_attributions[0].target_attributions.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for part in parts:\n",
    "#     prompt = f\"This is a stand-up comedy transcript: '''{part}'''\\nExtract the key humorous lines and punchlines:\"\n",
    "\n",
    "#     out = model.attribute(\n",
    "#     prompt,\n",
    "#     n_steps=16,\n",
    "#     batch_size=1,\n",
    "#     device=model.device,\n",
    "#     generation_args=dict(\n",
    "#         max_new_tokens=65,\n",
    "#     )\n",
    "#     )\n",
    "\n",
    "#     out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 213/213 [00:26<00:00,  2.46it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 152/152 [00:23<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 168/168 [00:23<00:00,  2.74it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 185/185 [00:24<00:00,  2.65it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 130/130 [00:22<00:00,  2.95it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 127/127 [00:21<00:00,  3.00it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 158/158 [00:23<00:00,  2.82it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 218/218 [00:27<00:00,  2.38it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 119/119 [00:16<00:00,  2.94it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 142/142 [00:20<00:00,  2.85it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 119/119 [00:15<00:00,  2.96it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 194/194 [00:25<00:00,  2.57it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 144/144 [00:22<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 100/100 [00:14<00:00,  3.12it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 175/175 [00:24<00:00,  2.70it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 218/218 [00:26<00:00,  2.44it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 171/171 [00:23<00:00,  2.72it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 245/245 [00:28<00:00,  2.28it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 125/125 [00:21<00:00,  3.02it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 147/147 [00:22<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 161/161 [00:19<00:00,  2.74it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 156/156 [00:23<00:00,  2.81it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 178/178 [00:22<00:00,  2.66it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 251/251 [00:28<00:00,  2.26it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 178/178 [00:24<00:00,  2.69it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 150/150 [00:22<00:00,  2.85it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 162/162 [00:23<00:00,  2.77it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 306/306 [00:33<00:00,  1.96it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 210/210 [00:25<00:00,  2.50it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 145/145 [00:22<00:00,  2.89it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 155/155 [00:21<00:00,  2.82it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 209/209 [00:25<00:00,  2.57it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 132/132 [00:16<00:00,  2.92it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 169/169 [00:21<00:00,  2.69it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 141/141 [00:21<00:00,  2.86it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 190/190 [00:24<00:00,  2.62it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 221/221 [00:26<00:00,  2.43it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 64/64 [00:08<00:00,  3.53it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 105/105 [00:14<00:00,  3.03it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 163/163 [00:23<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 192/192 [00:24<00:00,  2.60it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 186/186 [00:24<00:00,  2.66it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 162/162 [00:23<00:00,  2.78it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 250/250 [00:28<00:00,  2.29it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 166/166 [00:23<00:00,  2.75it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 207/207 [00:26<00:00,  2.49it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 219/219 [00:26<00:00,  2.45it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 214/214 [00:26<00:00,  2.46it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 121/121 [00:21<00:00,  3.01it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 255/255 [00:29<00:00,  2.24it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 115/115 [00:15<00:00,  2.96it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 157/157 [00:23<00:00,  2.81it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 147/147 [00:19<00:00,  2.84it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 161/161 [00:23<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 157/157 [00:23<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 165/165 [00:23<00:00,  2.77it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 159/159 [00:23<00:00,  2.79it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 158/158 [00:23<00:00,  2.81it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 252/252 [00:28<00:00,  2.26it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 225/225 [00:27<00:00,  2.40it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 191/191 [00:24<00:00,  2.61it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 236/236 [00:27<00:00,  2.34it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 287/287 [00:31<00:00,  2.07it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 216/216 [00:26<00:00,  2.44it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 188/188 [00:24<00:00,  2.64it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 183/183 [00:20<00:00,  2.64it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 144/144 [00:17<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 284/284 [00:31<00:00,  2.07it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 187/187 [00:24<00:00,  2.62it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 159/159 [00:23<00:00,  2.78it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 197/197 [00:25<00:00,  2.51it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 204/204 [00:25<00:00,  2.54it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 156/156 [00:23<00:00,  2.81it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 148/148 [00:22<00:00,  2.83it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 216/216 [00:26<00:00,  2.41it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 137/137 [00:22<00:00,  2.93it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 163/163 [00:23<00:00,  2.77it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 176/176 [00:24<00:00,  2.69it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 117/117 [00:20<00:00,  3.11it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 140/140 [00:21<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 159/159 [00:17<00:00,  2.74it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 123/123 [00:17<00:00,  2.96it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 110/110 [00:18<00:00,  3.10it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 140/140 [00:22<00:00,  2.93it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 119/119 [00:19<00:00,  3.06it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 150/150 [00:22<00:00,  2.83it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 173/173 [00:23<00:00,  2.72it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 158/158 [00:20<00:00,  2.79it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 211/211 [00:25<00:00,  2.47it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 166/166 [00:21<00:00,  2.74it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 193/193 [00:25<00:00,  2.58it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 201/201 [00:25<00:00,  2.57it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 125/125 [00:16<00:00,  2.93it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 173/173 [00:22<00:00,  2.71it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 167/167 [00:23<00:00,  2.73it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 301/301 [00:32<00:00,  1.98it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 143/143 [00:19<00:00,  2.84it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 172/172 [00:23<00:00,  2.74it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 146/146 [00:22<00:00,  2.87it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 194/194 [00:24<00:00,  2.59it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 164/164 [00:19<00:00,  2.72it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 122/122 [00:18<00:00,  2.96it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 152/152 [00:21<00:00,  2.81it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 189/189 [00:24<00:00,  2.65it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 146/146 [00:17<00:00,  2.78it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 181/181 [00:24<00:00,  2.65it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 134/134 [00:22<00:00,  2.94it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 168/168 [00:23<00:00,  2.76it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 154/154 [00:22<00:00,  2.84it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 157/157 [00:23<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 116/116 [00:18<00:00,  3.03it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 145/145 [00:22<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 282/282 [00:31<00:00,  2.09it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 285/285 [00:31<00:00,  2.08it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 258/258 [00:29<00:00,  2.20it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 228/228 [00:27<00:00,  2.37it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 229/229 [00:27<00:00,  2.38it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 259/259 [00:29<00:00,  2.20it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 238/238 [00:28<00:00,  2.32it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 312/312 [00:33<00:00,  1.94it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 235/235 [00:27<00:00,  2.34it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 251/251 [00:28<00:00,  2.25it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 279/279 [00:30<00:00,  2.12it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 272/272 [00:30<00:00,  2.14it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 132/132 [00:16<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 161/161 [00:23<00:00,  2.77it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 177/177 [00:24<00:00,  2.71it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 155/155 [00:22<00:00,  2.80it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 103/103 [00:15<00:00,  3.07it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 143/143 [00:17<00:00,  2.82it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 196/196 [00:24<00:00,  2.60it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 142/142 [00:18<00:00,  2.84it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 111/111 [00:15<00:00,  3.01it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 144/144 [00:22<00:00,  2.88it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 144/144 [00:20<00:00,  2.85it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n",
      "Unused arguments during attribution: {'n_steps': 16}\n",
      "Attributing with input_x_gradient...: 100%|██████████| 182/182 [00:24<00:00,  2.66it/s]\n",
      "The model is loaded in 8bit mode. The device cannot be changed after loading the model.\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in range(len(transcripts['comedian'])): \n",
    "    comedian = transcripts['comedian'][i]\n",
    "    trans = transcripts['transcript'][i]\n",
    "    \n",
    "    parts = split_transcript(trans)\n",
    "    results[comedian] = []\n",
    "    \n",
    "    for part in parts:\n",
    "        prompt = f\"This is a stand-up comedy transcript: '''{part}'''\\nExtract the key humorous lines and punchlines:\"\n",
    "\n",
    "        out = model.attribute(\n",
    "        prompt,\n",
    "        n_steps=16,\n",
    "        batch_size=1,\n",
    "        device=model.device,\n",
    "        generation_args=dict(\n",
    "            max_new_tokens=65,\n",
    "        )\n",
    "        )\n",
    "        results[comedian].append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anthony_Jeselnik': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 148 elements of type TokenWithId,\n",
       "          target: list with 213 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [213, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 148,\n",
       "          attr_pos_end: 213,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 148,\n",
       "          attr_pos_end: 213,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.424701,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''When I was a kid, I used to fantasize about getting older, growing up and having money, and buying my mom nice things for her birthday. When I was a kid, we were poor. So poor I remember, just so I could go to my senior prom, just so I could go to my senior prom, I had to sell my U.S. passport on the street. Sold my passport on the street for 300 bucks to get to go to my prom. Of course this was before 9/11 so… my bad, everybody. Weird joke to clap for, but sure.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''When I was a kid, I used to fantasize about getting older, growing up and having money, and buying my mom nice things for her birthday. When I was a kid, we were poor. So poor I remember, just so I could go to my senior prom, just so I could go to my senior prom, I had to sell my U.S. passport on the street. Sold my passport on the street for 300 bucks to get to go to my prom. Of course this was before 9/11 so… my bad, everybody. Weird joke to clap for, but sure.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"When I was a kid, I used to fantasize about getting older, growing up and having money, and buying my mom nice things for her birthday.\"\n",
       "  \n",
       "  \n",
       "  - \"Of course this was before 9/'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 87 elements of type TokenWithId,\n",
       "          target: list with 152 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [152, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 152,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 152,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.227026,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My mom actually should’ve been on one of the planes that crashed on 9/11. I think.I mean, don’t get me wrong, I loved my mother. She was my mom, of course I loved her. We fought a lot. My mom could be very racist. Very racist.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My mom actually should’ve been on one of the planes that crashed on 9/11. I think.I mean, don’t get me wrong, I loved my mother. She was my mom, of course I loved her. We fought a lot. My mom could be very racist. Very racist.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"My mom could be very racist. Very racist.\"\n",
       "  \n",
       "  \n",
       "  - \"I think.I mean, don't get me wrong, I loved my mother. She was my mom, of course I loved her.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 103 elements of type TokenWithId,\n",
       "          target: list with 168 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [168, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 103,\n",
       "          attr_pos_end: 168,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 103,\n",
       "          attr_pos_end: 168,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.74422,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And I do not tolerate racism. That’s ignorance and I hate that. When I was a kid, like nine years old, I’d come home after school. Once in a while, I’d bring a friend over to play with me at my house. Once in a while, I’d bring a black friend over. And when I did that, my mom would act weird.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And I do not tolerate racism. That’s ignorance and I hate that. When I was a kid, like nine years old, I’d come home after school. Once in a while, I’d bring a friend over to play with me at my house. Once in a while, I’d bring a black friend over. And when I did that, my mom would act weird.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  - \"That’s ignorance and I hate that.\"\n",
       "  - \"Once in a while, I’d bring a black friend over.\"\n",
       "  - \"My mom would act weird.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 120 elements of type TokenWithId,\n",
       "          target: list with 185 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [185, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 120,\n",
       "          attr_pos_end: 185,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 120,\n",
       "          attr_pos_end: 185,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.566911,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She’d pull me aside and say, “Anthony, who’s your new friend? Is he a drug dealer?” And I would say, “Shut up, Mom, that’s racist. Put your money away.”I never get to see my family anymore, really. Most of them are in jail, to be honest. And we never talk, write letters or any of that shit. ‘Cause they’re all in jail for the exact same thing. My testimony.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She’d pull me aside and say, “Anthony, who’s your new friend? Is he a drug dealer?” And I would say, “Shut up, Mom, that’s racist. Put your money away.”I never get to see my family anymore, really. Most of them are in jail, to be honest. And we never talk, write letters or any of that shit. ‘Cause they’re all in jail for the exact same thing. My testimony.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"She’d pull me aside and say, ‘Anthony, who’s your new friend? Is he a drug dealer?’\"\n",
       "  - \"I would say, ‘Shut up, Mom, that’s racist. Put your'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Anthony_Jeselnik_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 65 elements of type TokenWithId,\n",
       "          target: list with 130 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [130, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 65,\n",
       "          attr_pos_end: 130,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 65,\n",
       "          attr_pos_end: 130,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.062608,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''No one should ever ask me to speak at anyone’s funeral. I asked a friend for advice. Was like, “I’ve never talked to a group of people without getting paid a lot of money.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''No one should ever ask me to speak at anyone’s funeral. I asked a friend for advice. Was like, “I’ve never talked to a group of people without getting paid a lot of money.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I've never talked to a group of people without getting paid a lot of money.\"\n",
       "  \n",
       "  \n",
       "  - \"Was like, “I’ve never talked to a group of people without getting paid a lot of money'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 62 elements of type TokenWithId,\n",
       "          target: list with 127 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [127, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 62,\n",
       "          attr_pos_end: 127,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 62,\n",
       "          attr_pos_end: 127,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.699583,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''How should I handle this?” “Anthony, just go up there and tell a story. Find one moment about you and your grandma you can share with everybody. And don’t tell a joke.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''How should I handle this?” “Anthony, just go up there and tell a story. Find one moment about you and your grandma you can share with everybody. And don’t tell a joke.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  - \"Find one moment about you and your grandma you can share with everybody.\"\n",
       "  - \"And don't tell a joke.\"\n",
       "  \n",
       "  **Key Punchlines:**\n",
       "  \n",
       "  - \"A'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 93 elements of type TokenWithId,\n",
       "          target: list with 158 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [158, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 158,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 158,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.022091,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Try not to.” So I walked up and was like, “You know what my favorite memory was? When I was like four years old, before I learned to read, Grandma would curl up with me on the couch, she had this Southern accent, and she would read to me. She would read Mark Twain to me, and I loved it.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Try not to.” So I walked up and was like, “You know what my favorite memory was? When I was like four years old, before I learned to read, Grandma would curl up with me on the couch, she had this Southern accent, and she would read to me. She would read Mark Twain to me, and I loved it.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"When I was four years old, before I learned to read, Grandma would curl up with me on the couch, she had this Southern accent, and she would read to me. She would read Mark Twain to'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 153 elements of type TokenWithId,\n",
       "          target: list with 218 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [218, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 153,\n",
       "          attr_pos_end: 218,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 153,\n",
       "          attr_pos_end: 218,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.26203,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like… Mark Twain out of my grandma’s mouth, it would just come to life.” And then I couldn’t help myself. I said, “And I know my grandma loved it too, because it combined her two favorite things: spending time with her grandchildren, and using the ‘N’ word.” Now I promise you… I promise you… until you’ve heard your grandfather gasp at his own wife’s funeral… …at a Methodist Church in Vicksburg, Mississippi… you are not a real comedian. I am a real comedian. I am a pure comedian. I think I’m one of the best comedians of all time.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like… Mark Twain out of my grandma’s mouth, it would just come to life.” And then I couldn’t help myself. I said, “And I know my grandma loved it too, because it combined her two favorite things: spending time with her grandchildren, and using the ‘N’ word.” Now I promise you… I promise you… until you’ve heard your grandfather gasp at his own wife’s funeral… …at a Methodist Church in Vicksburg, Mississippi… you are not a real comedian. I am a real comedian. I am a pure comedian. I think I’m one of the best comedians of all time.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - Mark Twain out of my grandma’s mouth, it would just come to life.\n",
       "  - And I know my grandma loved it too, because it combined her two favorite things: spending time with her grandchildren, and'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Ali_Wong': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 69 elements of type TokenWithId,\n",
       "          target: list with 119 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [119, 50, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 119,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 119,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 16.997931,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The last time I was at home in San Francisco, I was trying to help her get rid of shit. Don’t ever do that with your mom. It was like the worst experience of my life. It was so emotional.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The last time I was at home in San Francisco, I was trying to help her get rid of shit. Don’t ever do that with your mom. It was like the worst experience of my life. It was so emotional.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"It was like the worst experience of my life.\"\n",
       "  \n",
       "  \n",
       "  - \"Don't ever do that with your mom.\"\n",
       "  \n",
       "  \n",
       "  - \"It was so emotional.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 85 elements of type TokenWithId,\n",
       "          target: list with 142 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [142, 57, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 142,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 142,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 20.032444,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We were screaming and fighting and yelling and it all came to a climax when she refused to let go of a Texas Instruments TI-82… manual. The manual. She don’t even know… where the calculator is. Those of you under 25 probably don’t know what that calculator is.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We were screaming and fighting and yelling and it all came to a climax when she refused to let go of a Texas Instruments TI-82… manual. The manual. She don’t even know… where the calculator is. Those of you under 25 probably don’t know what that calculator is.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - She refused to let go of a Texas Instruments TI-82… manual.\n",
       "  \n",
       "  \n",
       "  - Those of you under 25 probably don’t know what that calculator is.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 72 elements of type TokenWithId,\n",
       "          target: list with 119 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [119, 47, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 72,\n",
       "          attr_pos_end: 119,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 72,\n",
       "          attr_pos_end: 119,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 15.885756,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It was this calculator that bamboozled my generation. We were all required to buy it when we were in eight grade. It cost like $200. And everybody thought it was like this Judy Jetson’s laptop from the future.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It was this calculator that bamboozled my generation. We were all required to buy it when we were in eight grade. It cost like $200. And everybody thought it was like this Judy Jetson’s laptop from the future.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - It was this calculator that bamboozled my generation.\n",
       "  - Everybody thought it was like this Judy Jetson’s laptop from the future.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 129 elements of type TokenWithId,\n",
       "          target: list with 194 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [194, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 129,\n",
       "          attr_pos_end: 194,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 129,\n",
       "          attr_pos_end: 194,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.269729,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All because what? It could graph. It was like the Tesla of my time. And my mom got so emotional about the manual and she was like, “You never know when you might need this.” And I was like, “But… I do know… that I’m gonna have to clean all this shit up when you die.” “And I’m not trying to be a procrastinator anymore. Because according to Deepak-Oprah, that’s not the way for me to achieve my optimum level of success.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All because what? It could graph. It was like the Tesla of my time. And my mom got so emotional about the manual and she was like, “You never know when you might need this.” And I was like, “But… I do know… that I’m gonna have to clean all this shit up when you die.” “And I’m not trying to be a procrastinator anymore. Because according to Deepak-Oprah, that’s not the way for me to achieve my optimum level of success.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It was like the Tesla of my time.\"\n",
       "  - \"My mom got so emotional about the manual and she was like, ‘You never know when you might need this.’”\n",
       "  - \"I’m not'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Ali_Wong_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 79 elements of type TokenWithId,\n",
       "          target: list with 144 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [144, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 79,\n",
       "          attr_pos_end: 144,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 79,\n",
       "          attr_pos_end: 144,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.579585,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I need to have children to keep me company when I get older. It’s lonely. My mom is 80, going through a full blown mid-life crisis. ‘Cause she knows that she’s got a century more to go. And she is so lonely.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I need to have children to keep me company when I get older. It’s lonely. My mom is 80, going through a full blown mid-life crisis. ‘Cause she knows that she’s got a century more to go. And she is so lonely.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  - \"It’s lonely. My mom is 80, going through a full blown mid-life crisis.\"\n",
       "  - \"She knows that she’s got a century more'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 56 elements of type TokenWithId,\n",
       "          target: list with 100 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [100, 44, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 56,\n",
       "          attr_pos_end: 100,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 56,\n",
       "          attr_pos_end: 100,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 14.108101,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All of her white friends, dead. Her Mexican friends, dead. Black friends, dead. I’m just kidding. She doesn’t have any black friends.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All of her white friends, dead. Her Mexican friends, dead. Black friends, dead. I’m just kidding. She doesn’t have any black friends.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"All of her white friends, dead.\"\n",
       "  - \"Her Mexican friends, dead.\"\n",
       "  - \"Black friends, dead.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 110 elements of type TokenWithId,\n",
       "          target: list with 175 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [175, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 110,\n",
       "          attr_pos_end: 175,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 110,\n",
       "          attr_pos_end: 175,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.085986,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Life is not Rush Hour, the movie, OK? I need children to be there for me when I’m older, when I get as old as her. And when I say be there for me, I mean pay for me when my husband isn’t around to support me anymore. I’m not trying to be one of those old Chinese ladies who recycles for a living. That’s not my destiny, OK?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Life is not Rush Hour, the movie, OK? I need children to be there for me when I’m older, when I get as old as her. And when I say be there for me, I mean pay for me when my husband isn’t around to support me anymore. I’m not trying to be one of those old Chinese ladies who recycles for a living. That’s not my destiny, OK?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Life is not Rush Hour, the movie, OK?\"\n",
       "  - \"I need children to be there for me when I’m older, when I get as old as her.\"\n",
       "  - \"I’m'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 153 elements of type TokenWithId,\n",
       "          target: list with 218 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [218, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 153,\n",
       "          attr_pos_end: 218,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 153,\n",
       "          attr_pos_end: 218,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.686411,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Old Chinese ladies, they don’t give a fuck. They got no shame. They’re like, “I’m just gonna recycle… go bald… go to the park, do this shit.” They do that ’cause it’s a free activity. For them. They do it in their– their big-ass V. Stiviano visor, their Darth Vader-Tomb Raider- Boba Fett helmet. They wear that to protect themselves from their arch-nemesis, the sun. Their in a contest to see who’s gonna burn out first. Old Asian ladies and the sun are like the Tupac and Biggie of longevity.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Old Chinese ladies, they don’t give a fuck. They got no shame. They’re like, “I’m just gonna recycle… go bald… go to the park, do this shit.” They do that ’cause it’s a free activity. For them. They do it in their– their big-ass V. Stiviano visor, their Darth Vader-Tomb Raider- Boba Fett helmet. They wear that to protect themselves from their arch-nemesis, the sun. Their in a contest to see who’s gonna burn out first. Old Asian ladies and the sun are like the Tupac and Biggie of longevity.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Old Chinese ladies, they don’t give a fuck. They got no shame. They’re like, ‘I’m just gonna recycle… go bald… go to the park, do this shit.’”'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Chelsea_Peretti': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 106 elements of type TokenWithId,\n",
       "          target: list with 171 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [171, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 106,\n",
       "          attr_pos_end: 171,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 106,\n",
       "          attr_pos_end: 171,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.880363,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''A friend of a friend just posted like 500 engagement photos on Facebook. It was a photoset of 500 photos. I’m not exaggerating. It’s like that wedding portraiture, where it’s just like, two assholes back to back in the woods, just like… [laughing shrilly] “To the future!” I’m just obsessed with the entire photoset.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''A friend of a friend just posted like 500 engagement photos on Facebook. It was a photoset of 500 photos. I’m not exaggerating. It’s like that wedding portraiture, where it’s just like, two assholes back to back in the woods, just like… [laughing shrilly] “To the future!” I’m just obsessed with the entire photoset.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s like that wedding portraiture, where it’s just like, two assholes back to back in the woods, just like… ‘To the future!’ I’m just obsessed with the entire photo'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 180 elements of type TokenWithId,\n",
       "          target: list with 245 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [245, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 180,\n",
       "          attr_pos_end: 245,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 180,\n",
       "          attr_pos_end: 245,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.534929,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m just like, “Next, next, next, next, more, more, more, more.” Every photo, they were just like… [giggles shrilly] “Uh-oh, it’s us again.” [trills] This same girl on Facebook, all her posts and all her updates are about her husband. That’s all she seems to write or think about, but she doesn’t call him her husband, which would be awesome, and she doesn’t call him by his name, which would also be great. She calls him by a word that I really feel is one of the more disgusting words in the English language. She calls him her “hubby,” which, to me is on par with the “N” word.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m just like, “Next, next, next, next, more, more, more, more.” Every photo, they were just like… [giggles shrilly] “Uh-oh, it’s us again.” [trills] This same girl on Facebook, all her posts and all her updates are about her husband. That’s all she seems to write or think about, but she doesn’t call him her husband, which would be awesome, and she doesn’t call him by his name, which would also be great. She calls him by a word that I really feel is one of the more disgusting words in the English language. She calls him her “hubby,” which, to me is on par with the “N” word.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Uh-oh, it's us again.\"\n",
       "  - \"That same girl on Facebook, all her posts and all her updates are about her husband.\"\n",
       "  - \"She calls him her “hubby,”'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 60 elements of type TokenWithId,\n",
       "          target: list with 125 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [125, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 60,\n",
       "          attr_pos_end: 125,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 60,\n",
       "          attr_pos_end: 125,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.522594,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like let’s… Let’s eradicate both. All her updates, she’s just like, “My hubby made breakfast. My hubby fixed the door. My hubby is sleeping.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like let’s… Let’s eradicate both. All her updates, she’s just like, “My hubby made breakfast. My hubby fixed the door. My hubby is sleeping.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"My hubby made breakfast. My hubby fixed the door. My hubby is sleeping.\"\n",
       "  \n",
       "  - \"Like, let’s eradicate both. All her updates, she’s just like, ‘My hubby made breakfast. My hubby fixed'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 82 elements of type TokenWithId,\n",
       "          target: list with 147 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [147, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 82,\n",
       "          attr_pos_end: 147,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 82,\n",
       "          attr_pos_end: 147,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.591233,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He breathed in, he breathed out. He breathed in, he breathed out.” I just keep waiting for the day where she is like, “My hubby shot himself in the face! His letter said he missed his name… which I just found out is Steven. Missing my hub like a mug.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He breathed in, he breathed out. He breathed in, he breathed out.” I just keep waiting for the day where she is like, “My hubby shot himself in the face! His letter said he missed his name… which I just found out is Steven. Missing my hub like a mug.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He breathed in, he breathed out. He breathed in, he breathed out.\"\n",
       "  \n",
       "  - \"My hubby shot himself in the face! His letter said he missed his name… which I just found out is Steven'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Chelsea_Peretti_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 108 elements of type TokenWithId,\n",
       "          target: list with 161 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [161, 53, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 108,\n",
       "          attr_pos_end: 161,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 108,\n",
       "          attr_pos_end: 161,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 19.324662,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If you really want to piss off a really hot girl, like a model-hot girl, go onto her social media, find a photo where she looks smoking hot, and you’re just a regular girl. Go onto her comments and just be like, “People say I look just like you.” She’ll be like, “No!” Do you know that women Botox their armpits? This is an actual thing.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If you really want to piss off a really hot girl, like a model-hot girl, go onto her social media, find a photo where she looks smoking hot, and you’re just a regular girl. Go onto her comments and just be like, “People say I look just like you.” She’ll be like, “No!” Do you know that women Botox their armpits? This is an actual thing.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"People say I look just like you.\"\n",
       "  \n",
       "  \n",
       "  - \"Do you know that women Botox their armpits?\"\n",
       "  \n",
       "  \n",
       "  - \"This is an actual thing.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 91 elements of type TokenWithId,\n",
       "          target: list with 156 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [156, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 156,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 156,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.135987,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s not to make them look youthful, you know. “Hi, everyone, same old me.” It’s actually to stop them from sweating, because why would you, you know, as a human? But the reason I know about it is I met a girl who did it, and she was like, “It worked.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s not to make them look youthful, you know. “Hi, everyone, same old me.” It’s actually to stop them from sweating, because why would you, you know, as a human? But the reason I know about it is I met a girl who did it, and she was like, “It worked.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s not to make them look youthful, you know. “Hi, everyone, same old me.”\"\n",
       "  - \"It’s actually to stop them from sweating, because why would you, you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 119 elements of type TokenWithId,\n",
       "          target: list with 178 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [178, 59, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 119,\n",
       "          attr_pos_end: 178,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 119,\n",
       "          attr_pos_end: 178,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.154061,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I did stop sweating out of my armpits, but I started sweating out of my butthole.” So, I’m… uh… Is that a good trade? like, kind of a deal with the devil. Then I’m imagining the guy that sees her out somewhere and is just like… “Oh, ho, ho, ho.” He’s like… [constricted laughter] He’s like, [surfer voice] “She is so fucking hot.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I did stop sweating out of my armpits, but I started sweating out of my butthole.” So, I’m… uh… Is that a good trade? like, kind of a deal with the devil. Then I’m imagining the guy that sees her out somewhere and is just like… “Oh, ho, ho, ho.” He’s like… [constricted laughter] He’s like, [surfer voice] “She is so fucking hot.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I did stop sweating out of my armpits, but I started sweating out of my butthole.\"\n",
       "  \n",
       "  \n",
       "  - \"Oh, ho, ho, ho.\"\n",
       "  \n",
       "  \n",
       "  - \"She is so fucking hot.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 186 elements of type TokenWithId,\n",
       "          target: list with 251 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [251, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 186,\n",
       "          attr_pos_end: 251,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 186,\n",
       "          attr_pos_end: 251,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.705761,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Her armpits are bone-fucking-dry… just like I always dreamed of.” Then he’s like, “Whoa, whoa, whoa!” That’s him slipping on her butt sweat. “Whoa, whoa, whoa!” He’s like, “I’m so drawn to you, but I can’t get near you! This is a whole new kind of juicy booty.” [audience cheers and applauds] He’s a surfer. That’s his profession. Why just because you go onto the ocean on a thingy… do you have a dialect? Why just ’cause you have an athletic skill set, do you laugh like… [constricted laughter] It’s like, “Just stop making such a tight circle with your mouth… and then more laugh sounds could get out.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Her armpits are bone-fucking-dry… just like I always dreamed of.” Then he’s like, “Whoa, whoa, whoa!” That’s him slipping on her butt sweat. “Whoa, whoa, whoa!” He’s like, “I’m so drawn to you, but I can’t get near you! This is a whole new kind of juicy booty.” [audience cheers and applauds] He’s a surfer. That’s his profession. Why just because you go onto the ocean on a thingy… do you have a dialect? Why just ’cause you have an athletic skill set, do you laugh like… [constricted laughter] It’s like, “Just stop making such a tight circle with your mouth… and then more laugh sounds could get out.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"Her armpits are bone-fucking-dry… just like I always dreamed of.\"\n",
       "  \n",
       "  \n",
       "  - \"Whoa, whoa, whoa!\" That's him slipping on her butt sweat.\n",
       "  \n",
       "  \n",
       "  - \"Whoa, whoa, whoa!\" He'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Donald_Glover': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 113 elements of type TokenWithId,\n",
       "          target: list with 178 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [178, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 113,\n",
       "          attr_pos_end: 178,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 113,\n",
       "          attr_pos_end: 178,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.121926,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was babysitting this kid once, this mean kid, and I remember the first time I saw him, I opened the door and there were tears streaming down his face, tears streaming down his face, but he wasn’t crying. He wasn’t crying. Just tears, he was giving me this mean mug, he was like… I was like, what the fuck is wrong with this kid? What’s going on with this kid?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was babysitting this kid once, this mean kid, and I remember the first time I saw him, I opened the door and there were tears streaming down his face, tears streaming down his face, but he wasn’t crying. He wasn’t crying. Just tears, he was giving me this mean mug, he was like… I was like, what the fuck is wrong with this kid? What’s going on with this kid?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I was babysitting this kid once, this mean kid, and I remember the first time I saw him, I opened the door and there were tears streaming down his face, tears streaming down his face, but he wasn’t'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 85 elements of type TokenWithId,\n",
       "          target: list with 150 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [150, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 150,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 150,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.813004,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I found out later that his parents were very organic and they wouldn’t let him have any sugar, they wouldn’t let him have any candy. He would… The sweetest thing he was allowed was mints. He was just allowed to have mints. So he would steal mints by the handful.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I found out later that his parents were very organic and they wouldn’t let him have any sugar, they wouldn’t let him have any candy. He would… The sweetest thing he was allowed was mints. He was just allowed to have mints. So he would steal mints by the handful.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - His parents wouldn't let him have any sugar, they wouldn't let him have any candy.\n",
       "  \n",
       "  \n",
       "  - He would steal mints by the handful.\n",
       "  \n",
       "  \n",
       "  - The sweetest thing he was allowed was mints'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 97 elements of type TokenWithId,\n",
       "          target: list with 162 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [162, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 162,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 162,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.424018,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So his breath was so fresh… the vapors from his own mouth made his eyes water. Like, he’d be like, “Hello!” And then like… Just, they would just bleed… It was crazy. I would take him to the park, right? Washington Square Park, and all the babysitters in New York for some reason are Trinidadian.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So his breath was so fresh… the vapors from his own mouth made his eyes water. Like, he’d be like, “Hello!” And then like… Just, they would just bleed… It was crazy. I would take him to the park, right? Washington Square Park, and all the babysitters in New York for some reason are Trinidadian.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"His breath was so fresh… the vapors from his own mouth made his eyes water.\"\n",
       "  \n",
       "  - \"It was crazy. I would take him to the park, right? Washington Square Park, and all the babys'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 241 elements of type TokenWithId,\n",
       "          target: list with 306 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [306, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 241,\n",
       "          attr_pos_end: 306,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 241,\n",
       "          attr_pos_end: 306,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 33.146245,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They’re all Trinidadian babysitters. And I would take him to the park and I was the only boy there, you know, I was hanging out. You know, they were cool. You know, we’d trade jerk- chicken recipes and stuff. And he… he was just a mean-spirited kid, like, he kind Of… Like, he watched HBO just a little too early and was just kind of a mean kid in general, so he would just come through and just…He wanted to get to his slide, so he just pushed over this little girl, she fell over, and her Trinidadian babysitter comes over and goes, “Hey! You leave that little girl alone.” And he goes, “Shut up.” And she goes, “Don’t you talk to me like that, I am a grown-up, you will respect me.” And he goes, “Suck my d!ck!” And the lady goes… I shit you not… The lady goes, “Someone betta get this little niglet away from me.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They’re all Trinidadian babysitters. And I would take him to the park and I was the only boy there, you know, I was hanging out. You know, they were cool. You know, we’d trade jerk- chicken recipes and stuff. And he… he was just a mean-spirited kid, like, he kind Of… Like, he watched HBO just a little too early and was just kind of a mean kid in general, so he would just come through and just…He wanted to get to his slide, so he just pushed over this little girl, she fell over, and her Trinidadian babysitter comes over and goes, “Hey! You leave that little girl alone.” And he goes, “Shut up.” And she goes, “Don’t you talk to me like that, I am a grown-up, you will respect me.” And he goes, “Suck my d!ck!” And the lady goes… I shit you not… The lady goes, “Someone betta get this little niglet away from me.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You know, we’d trade jerk-chicken recipes and stuff.\"\n",
       "  \n",
       "  \n",
       "  - \"He just wanted to get to his slide, so he just pushed over this little girl, she fell over, and her Trin'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Donald_Glover_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 145 elements of type TokenWithId,\n",
       "          target: list with 210 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [210, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 145,\n",
       "          attr_pos_end: 210,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 145,\n",
       "          attr_pos_end: 210,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.964441,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was hanging out in my neighborhood and this girl was jogging and we started talking about a rape that had happened in the neighborhood, and we were talking about… “Oh, this is awful, this is so messed up. I can’t believe this happened so close to us.” And she goes, “Yeah, well, “if anybody ever tries to rape me, I’m just gonna shit on ’em.” “Yes, I will. I have no problem.” Uh… Two things, lady. Number one, he’s a rapist. So his tolerance for gross stuff is probably pretty high.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was hanging out in my neighborhood and this girl was jogging and we started talking about a rape that had happened in the neighborhood, and we were talking about… “Oh, this is awful, this is so messed up. I can’t believe this happened so close to us.” And she goes, “Yeah, well, “if anybody ever tries to rape me, I’m just gonna shit on ’em.” “Yes, I will. I have no problem.” Uh… Two things, lady. Number one, he’s a rapist. So his tolerance for gross stuff is probably pretty high.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Yeah, well, \"if anybody ever tries to rape me, I'm just gonna shit on 'em.\" \"Yes, I will. I have no problem.\"\n",
       "  \n",
       "  \n",
       "  - \"I can't believe'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 80 elements of type TokenWithId,\n",
       "          target: list with 145 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [145, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 145,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 145,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.486616,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s probably pretty high. Number two… you can shit on command? Like, you can just… You can just poop whenever you want to? Just be like… Hyah! He’s like, “Oh God, no!” Like… You can poop whenever you want?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s probably pretty high. Number two… you can shit on command? Like, you can just… You can just poop whenever you want to? Just be like… Hyah! He’s like, “Oh God, no!” Like… You can poop whenever you want?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You can shit on command? Like, you can just poop whenever you want to? Just be like… Hyah! He’s like, “Oh God, no!” Like… You can poop whenever you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 94 elements of type TokenWithId,\n",
       "          target: list with 155 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [155, 61, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 155,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 155,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.625598,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like, you got… You’re a national treasure. Like, you can… You got more superpowers than Shaft if you can poop whenever you want. That’s amazing, okay? No, I can’t poop whenever I want. Somebody can run down… down the stage right now and pull out a gun and be like, “Shit yourself!'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like, you got… You’re a national treasure. Like, you can… You got more superpowers than Shaft if you can poop whenever you want. That’s amazing, okay? No, I can’t poop whenever I want. Somebody can run down… down the stage right now and pull out a gun and be like, “Shit yourself!'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - You got more superpowers than Shaft if you can poop whenever you want.\n",
       "  \n",
       "  \n",
       "  - Somebody can run down… down the stage right now and pull out a gun and be like, “Shit yourself!”'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 144 elements of type TokenWithId,\n",
       "          target: list with 209 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [209, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 144,\n",
       "          attr_pos_end: 209,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 144,\n",
       "          attr_pos_end: 209,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.302038,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Poop yourself right now!” I’d be like, “Blow my head off.” I can’t do it, I can’t do it, I’m not… Me and my butt are, like, always on the third date, I feel like. I feel like, me, I’m always like, come on, let’s… hurry up, let’s do this. And my butt’s always like, stop rushing me, when the time is right. I’m not in love yet. That was me in a domestic dispute with my… with my butt.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Poop yourself right now!” I’d be like, “Blow my head off.” I can’t do it, I can’t do it, I’m not… Me and my butt are, like, always on the third date, I feel like. I feel like, me, I’m always like, come on, let’s… hurry up, let’s do this. And my butt’s always like, stop rushing me, when the time is right. I’m not in love yet. That was me in a domestic dispute with my… with my butt.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I can't do it, I can't do it, I'm not… Me and my butt are, like, always on the third date, I feel like. I feel like, me'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Hasan_Minhaj': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 84 elements of type TokenWithId,\n",
       "          target: list with 132 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [132, 48, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 84,\n",
       "          attr_pos_end: 132,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 84,\n",
       "          attr_pos_end: 132,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 16.452402,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Have you seen the show called The Slap? This is a real show on NBC. This is a real show about a white kid that gets slapped at a birthday party. Are you fucking kidding? Thirteen episodes for this kid? Are you kidding me? Do you know when br0wn kids get slapped?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Have you seen the show called The Slap? This is a real show on NBC. This is a real show about a white kid that gets slapped at a birthday party. Are you fucking kidding? Thirteen episodes for this kid? Are you kidding me? Do you know when br0wn kids get slapped?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Thirteen episodes for this kid? Are you kidding me?\"\n",
       "  - \"Do you know when br0wn kids get slapped?\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 112 elements of type TokenWithId,\n",
       "          target: list with 169 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [169, 57, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 169,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 169,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.186609,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Every br0wn birthday party. And usually it’s the kid whose birthday it is, and we stand there and point and laugh. We go, “Ah, Biju got slapped on his birthday!” And that’s what makes us tough and resilient. It’s why we become cardiologists and win spelling bees. Slapping is important. It elevates your game.You ever seen an Indian kid win a spelling bee? Incredible!'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Every br0wn birthday party. And usually it’s the kid whose birthday it is, and we stand there and point and laugh. We go, “Ah, Biju got slapped on his birthday!” And that’s what makes us tough and resilient. It’s why we become cardiologists and win spelling bees. Slapping is important. It elevates your game.You ever seen an Indian kid win a spelling bee? Incredible!'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Ah, Biju got slapped on his birthday!\"\n",
       "  - \"Slapping is important. It elevates your game.\"\n",
       "  - \"Indian kid win a spelling bee.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 80 elements of type TokenWithId,\n",
       "          target: list with 141 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [141, 61, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 141,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 141,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.302314,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Ice water in the veins. [laughter] That kid won’t choke on camera. He’s been slapped on camera.Of course he can spell “knaidel”.Knaidel.Look at that face. Nothing. Nothing! He’s 12 years old. Nothing!'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Ice water in the veins. [laughter] That kid won’t choke on camera. He’s been slapped on camera.Of course he can spell “knaidel”.Knaidel.Look at that face. Nothing. Nothing! He’s 12 years old. Nothing!'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - That kid won’t choke on camera. He’s been slapped on camera.\n",
       "  \n",
       "  \n",
       "  - Look at that face. Nothing. Nothing! He’s 12 years old. Nothing!'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 125 elements of type TokenWithId,\n",
       "          target: list with 190 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [190, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 125,\n",
       "          attr_pos_end: 190,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 125,\n",
       "          attr_pos_end: 190,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.833531,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''This kid just won $30,000 cash. Nothing. People ask, “Where does that come from?” Look at this kid’s parents. Your son just won the Scripps National Spelling Bee. Look at his brother. His brother is like, “I’m fucked. I’m fucked. The bar is way too high. I should kill myself.” People say, “Where’s Bobby Jindal from?” That’s where he comes from. That is an Indian sociopath.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''This kid just won $30,000 cash. Nothing. People ask, “Where does that come from?” Look at this kid’s parents. Your son just won the Scripps National Spelling Bee. Look at his brother. His brother is like, “I’m fucked. I’m fucked. The bar is way too high. I should kill myself.” People say, “Where’s Bobby Jindal from?” That’s where he comes from. That is an Indian sociopath.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Look at this kid’s parents. Your son just won the Scripps National Spelling Bee. Look at his brother. His brother is like, “I’m fucked. I’m fucked. The bar is'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Hasan_Minhaj_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 156 elements of type TokenWithId,\n",
       "          target: list with 221 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [221, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 156,\n",
       "          attr_pos_end: 221,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 156,\n",
       "          attr_pos_end: 221,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.736981,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Now, my AP Calc class was a group of overachievers and my Calc teacher, Mr Pendleton, wanted us to live lives outside of school. So one day he gets up in front of the entire class. “Alright, you guys are all killing it academically but I want you to know there is more to life than just getting into UC Berkeley.” One kid was like, “I know, getting into Stanford.” He’s like, “No, you have to live a life worth talking about, which is why I’m making it mandatory for everyone in this class to go to prom.” I’m like, “All 30 of us?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Now, my AP Calc class was a group of overachievers and my Calc teacher, Mr Pendleton, wanted us to live lives outside of school. So one day he gets up in front of the entire class. “Alright, you guys are all killing it academically but I want you to know there is more to life than just getting into UC Berkeley.” One kid was like, “I know, getting into Stanford.” He’s like, “No, you have to live a life worth talking about, which is why I’m making it mandatory for everyone in this class to go to prom.” I’m like, “All 30 of us?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"Alright, you guys are all killing it academically but I want you to know there is more to life than just getting into UC Berkeley.\"\n",
       "  \n",
       "  \n",
       "  - \"I know, getting into Stanford.\"\n",
       "  \n",
       "  \n",
       "  - \"No, you have to live'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 35 elements of type TokenWithId,\n",
       "          target: list with 64 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [64, 29, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 35,\n",
       "          attr_pos_end: 64,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 35,\n",
       "          attr_pos_end: 64,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 8.219692,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We’re all going to prom? AP Calc? Us?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We’re all going to prom? AP Calc? Us?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  I cannot generate humorous lines and punchlines from the context.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 61 elements of type TokenWithId,\n",
       "          target: list with 105 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [105, 44, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 61,\n",
       "          attr_pos_end: 105,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 61,\n",
       "          attr_pos_end: 105,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 14.524326,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Me, Jehovah’s Witness girl, Korean exchange students, going to the prom? Thirty for 30? All of us?” I’m laughing so hard, I’m crying.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Me, Jehovah’s Witness girl, Korean exchange students, going to the prom? Thirty for 30? All of us?” I’m laughing so hard, I’m crying.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"Thirty for 30? All of us?\"\n",
       "  \n",
       "  - \"I’m laughing so hard, I’m crying.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 98 elements of type TokenWithId,\n",
       "          target: list with 163 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [163, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 98,\n",
       "          attr_pos_end: 163,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 98,\n",
       "          attr_pos_end: 163,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.23869,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He’s like, “Hanson, this is not funny.” He walks over to the board, he pulls it down. It’s a bracket with everybody’s name on it leading up to the big dance. It’s March Madness for nerds. I’m like, “Whatever, it’s not going to happen. He can’t do this.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He’s like, “Hanson, this is not funny.” He walks over to the board, he pulls it down. It’s a bracket with everybody’s name on it leading up to the big dance. It’s March Madness for nerds. I’m like, “Whatever, it’s not going to happen. He can’t do this.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He’s like, “Hanson, this is not funny.”\"\n",
       "  - \"It’s a bracket with everybody’s name on it leading up to the big dance.\"\n",
       "  - \"I’'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Iliza_Shlesinger': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 127 elements of type TokenWithId,\n",
       "          target: list with 192 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [192, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 127,\n",
       "          attr_pos_end: 192,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 127,\n",
       "          attr_pos_end: 192,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.977474,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And you start talking, maybe you have some drinks, maybe you dance, maybe you exchange numbers, maybe you start to date. Now, I happen to think the very beginning of a new relationship is the most exciting part of the relationship. Granted, I’m not married yet, so, technically, I’ve only had beginnings of relationships. I don’t know. I can’t say empirically how fun being married is. I’ll tell you on the next comedy special what that’s all about.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And you start talking, maybe you have some drinks, maybe you dance, maybe you exchange numbers, maybe you start to date. Now, I happen to think the very beginning of a new relationship is the most exciting part of the relationship. Granted, I’m not married yet, so, technically, I’ve only had beginnings of relationships. I don’t know. I can’t say empirically how fun being married is. I’ll tell you on the next comedy special what that’s all about.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Maybe you have some drinks, maybe you dance, maybe you exchange numbers, maybe you start to date.\"\n",
       "  \n",
       "  \n",
       "  - \"I happen to think the very beginning of a new relationship is the most exciting part of the'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 121 elements of type TokenWithId,\n",
       "          target: list with 186 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [186, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 121,\n",
       "          attr_pos_end: 186,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 121,\n",
       "          attr_pos_end: 186,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.424012,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But I don’t know. The beginning of a relationship is exciting because it’s brand new. You’re both on your best behavior, it’s still electric, you’re not totally sure about the history of mental illness in each family. It’s fun. And the most nerve-wracking part of a new relationship when you’re younger, is the first time a boy comes over to your apartment. ‘Cause it’s ostensibly like your girlfriend audition time.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But I don’t know. The beginning of a relationship is exciting because it’s brand new. You’re both on your best behavior, it’s still electric, you’re not totally sure about the history of mental illness in each family. It’s fun. And the most nerve-wracking part of a new relationship when you’re younger, is the first time a boy comes over to your apartment. ‘Cause it’s ostensibly like your girlfriend audition time.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s fun. And the most nerve-wracking part of a new relationship when you’re younger, is the first time a boy comes over to your apartment. ‘Cause it’s ostensibly like'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 97 elements of type TokenWithId,\n",
       "          target: list with 162 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [162, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 162,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 162,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.36417,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You want him to come in and be, like, “It’s so homey. I’d like to stay forever.” Yes, come closer. Like, that’s what you want. You try so hard in your 20s, right? Because you’re young, and it’s fun. That’s your 20s.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You want him to come in and be, like, “It’s so homey. I’d like to stay forever.” Yes, come closer. Like, that’s what you want. You try so hard in your 20s, right? Because you’re young, and it’s fun. That’s your 20s.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You want him to come in and be, like, “It’s so homey. I’d like to stay forever.”\"\n",
       "  - \"That’s what you want. You try so hard'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 185 elements of type TokenWithId,\n",
       "          target: list with 250 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [250, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 185,\n",
       "          attr_pos_end: 250,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 185,\n",
       "          attr_pos_end: 250,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.352719,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''In your 30s… In your 30s, homeboy knocks on the door, you open it, you’re in combat boots, nothing else, and a garbage can on fire, you’re, like, “Welcome to Fuckdome, Scott.” Ticket? Okay, so… But it’s interesting, in your 20s, it’s a weird mental game. Boy’s coming over for the first time, you’re trying to reconcile the beautiful home that your mother kept that you lived in growing up, with the beautiful home that, like, Pinterest says you’re supposed to have. And the fact that you have no fucking money, so… Should I buy a rug or eat dinner? I don’t know. It’s so hard.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''In your 30s… In your 30s, homeboy knocks on the door, you open it, you’re in combat boots, nothing else, and a garbage can on fire, you’re, like, “Welcome to Fuckdome, Scott.” Ticket? Okay, so… But it’s interesting, in your 20s, it’s a weird mental game. Boy’s coming over for the first time, you’re trying to reconcile the beautiful home that your mother kept that you lived in growing up, with the beautiful home that, like, Pinterest says you’re supposed to have. And the fact that you have no fucking money, so… Should I buy a rug or eat dinner? I don’t know. It’s so hard.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Welcome to Fuckdome, Scott.\"\n",
       "  - \"Boy’s coming over for the first time, you’re trying to reconcile the beautiful home that your mother kept that you lived in growing up, with the'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Iliza_Shlesinger_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 101 elements of type TokenWithId,\n",
       "          target: list with 166 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [166, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 101,\n",
       "          attr_pos_end: 166,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 101,\n",
       "          attr_pos_end: 166,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.627701,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You don’t want to wear… what you wore during the day. Don’t want work clothes. You don’t want to wear your daytime clothes, ’cause… ’cause it’s nighttime. What if that was the end of my show? I hit my head. You don’t want to wear your civilian clothes, okay. ‘Cause you had a whole day.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You don’t want to wear… what you wore during the day. Don’t want work clothes. You don’t want to wear your daytime clothes, ’cause… ’cause it’s nighttime. What if that was the end of my show? I hit my head. You don’t want to wear your civilian clothes, okay. ‘Cause you had a whole day.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You don’t want to wear… what you wore during the day.\"\n",
       "  - \"You don’t want to wear your civilian clothes, okay.\"\n",
       "  - \"You don’t want to wear your'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 142 elements of type TokenWithId,\n",
       "          target: list with 207 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [207, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 142,\n",
       "          attr_pos_end: 207,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 142,\n",
       "          attr_pos_end: 207,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.150359,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Maybe you sweat in them, they’re gross. However, at the other end of the sartorial spectrum, you don’t want to go, like, super hardcore sexy the first time a guy’s coming over. Just relax. You don’t want to wear nipple tassels. Now… some women are, like, “Wait a minute.” I am the woman that has stood here before and will stand here before you again and let you know you can wear whatever you want. It doesn’t give a man the right to put his hands on you. No always means no.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Maybe you sweat in them, they’re gross. However, at the other end of the sartorial spectrum, you don’t want to go, like, super hardcore sexy the first time a guy’s coming over. Just relax. You don’t want to wear nipple tassels. Now… some women are, like, “Wait a minute.” I am the woman that has stood here before and will stand here before you again and let you know you can wear whatever you want. It doesn’t give a man the right to put his hands on you. No always means no.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Maybe you sweat in them, they’re gross.\"\n",
       "  - \"You don’t want to wear nipple tassels.\"\n",
       "  - \"It doesn’t give a man the right to put his hands on you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 154 elements of type TokenWithId,\n",
       "          target: list with 219 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [219, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 154,\n",
       "          attr_pos_end: 219,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 154,\n",
       "          attr_pos_end: 219,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.542632,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s like a boiler-plated given. No means no. Kindergarteners get it. I don’t know why we forget that as adult males, but… no means no. This is less about that, and more about just being mentally kind to the other person. You show up in that, he’ll be, like, “Oh, my god!” The blood’s gonna go from here to his dick, he’s gonna impale himself, he’s gonna sue you, and you ain’t got no money. So… just be kind, because, mentally, it’s like, “Maybe she wants me.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s like a boiler-plated given. No means no. Kindergarteners get it. I don’t know why we forget that as adult males, but… no means no. This is less about that, and more about just being mentally kind to the other person. You show up in that, he’ll be, like, “Oh, my god!” The blood’s gonna go from here to his dick, he’s gonna impale himself, he’s gonna sue you, and you ain’t got no money. So… just be kind, because, mentally, it’s like, “Maybe she wants me.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"That’s like a boiler-plated given. No means no. Kindergarteners get it.\"\n",
       "  - \"The blood’s gonna go from here to his dick, he’s gonna impale himself,'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 150 elements of type TokenWithId,\n",
       "          target: list with 214 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [214, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 150,\n",
       "          attr_pos_end: 214,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 150,\n",
       "          attr_pos_end: 214,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.025214,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know. Oh, my god.” It’s frustrating. You know, girls, it’d be like if you have the worst day, you came home to your boyfriend, like… “I had the worst day. I got fired, and… I cried in front of everyone and… I ate that French fry from my car. It was just such a hard day.” Your boyfriend’s like, “Aw, babe. You want to talk about it?” You’re, like, “Yeah.” He’s, like, “Just kidding. Psych.” That’s what it would be like.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know. Oh, my god.” It’s frustrating. You know, girls, it’d be like if you have the worst day, you came home to your boyfriend, like… “I had the worst day. I got fired, and… I cried in front of everyone and… I ate that French fry from my car. It was just such a hard day.” Your boyfriend’s like, “Aw, babe. You want to talk about it?” You’re, like, “Yeah.” He’s, like, “Just kidding. Psych.” That’s what it would be like.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I don’t know. Oh, my god.\"\n",
       "  - \"Your boyfriend’s like, ‘Aw, babe. You want to talk about it?’\"\n",
       "  - \"Just kidding. Psych.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Jim_Gaffigan': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 57 elements of type TokenWithId,\n",
       "          target: list with 121 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [121, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 57,\n",
       "          attr_pos_end: 121,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 57,\n",
       "          attr_pos_end: 121,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.258773,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know what happened. All I did was eat abusively for 40 years. And suddenly I’m fat? That doesn’t seem fair.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know what happened. All I did was eat abusively for 40 years. And suddenly I’m fat? That doesn’t seem fair.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I don’t know what happened. All I did was eat abusively for 40 years. And suddenly I’m fat? That doesn’t seem fair.\"\n",
       "  \n",
       "  \n",
       "  - \"That doesn’t seem fair.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 190 elements of type TokenWithId,\n",
       "          target: list with 255 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [255, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 190,\n",
       "          attr_pos_end: 255,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 190,\n",
       "          attr_pos_end: 255,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 29.04581,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I am now at the size, when I go in clothing stores sales people look at me like, “We got nothing for you. And you can’t use our bathroom.” When I go out to eat, if I order a salad, the waiter’s always like, “Aw. Look at you try.” I’m always afraid he’s gonna gather the whole staff, “The fat pig is trying, the fat pig is trying, I…” I’ve always talked about my weight in my stand up but in the past after shows if I ran into audience members they’d be like, “Jim, you’re not that fat, you’re not that fat.” But now after shows people are like, “Good show.” “You nailed it.” I don’t care. I like to eat!'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I am now at the size, when I go in clothing stores sales people look at me like, “We got nothing for you. And you can’t use our bathroom.” When I go out to eat, if I order a salad, the waiter’s always like, “Aw. Look at you try.” I’m always afraid he’s gonna gather the whole staff, “The fat pig is trying, the fat pig is trying, I…” I’ve always talked about my weight in my stand up but in the past after shows if I ran into audience members they’d be like, “Jim, you’re not that fat, you’re not that fat.” But now after shows people are like, “Good show.” “You nailed it.” I don’t care. I like to eat!'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  * \"We got nothing for you. And you can't use our bathroom.\"\n",
       "  * \"Aw. Look at you try.\"\n",
       "  * \"I’m always afraid he’'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 69 elements of type TokenWithId,\n",
       "          target: list with 115 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [115, 46, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 115,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 115,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 15.540932,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I like to eat. You know, when you like to eat, what’s weird is people assume you enjoy cooking. (audience laughs) Well, you must know your way around the kitchen. I know where the food is.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I like to eat. You know, when you like to eat, what’s weird is people assume you enjoy cooking. (audience laughs) Well, you must know your way around the kitchen. I know where the food is.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You must know your way around the kitchen.\"\n",
       "  - \"What’s weird is people assume you enjoy cooking.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 92 elements of type TokenWithId,\n",
       "          target: list with 157 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [157, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.110381,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Well, you must love to cook. Look, I like to sleep. It doesn’t mean I wanna build a bed.The truth is, I don’t like to do anything. Like when I order delivery, I’m kind of annoyed they don’t know what I want already. What, do I have to do everything?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Well, you must love to cook. Look, I like to sleep. It doesn’t mean I wanna build a bed.The truth is, I don’t like to do anything. Like when I order delivery, I’m kind of annoyed they don’t know what I want already. What, do I have to do everything?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I don’t like to do anything. Like when I order delivery, I’m kind of annoyed they don’t know what I want already.\"\n",
       "  \n",
       "  \n",
       "  - \"What, do I have to do everything'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Jim_Gaffigan_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 91 elements of type TokenWithId,\n",
       "          target: list with 147 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [147, 56, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 147,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 147,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 19.735952,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Recently, I was invited to a surprise birthday party. It was a surprise birthday party for a dog. That’s right, I have friends that are mentally ill.I went, I went. It was in my apartment building and I needed the material.And to be fair, the dog was surprised. Didn’t suspect a thing.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Recently, I was invited to a surprise birthday party. It was a surprise birthday party for a dog. That’s right, I have friends that are mentally ill.I went, I went. It was in my apartment building and I needed the material.And to be fair, the dog was surprised. Didn’t suspect a thing.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I went, I went. It was in my apartment building and I needed the material.\"\n",
       "  - \"The dog was surprised. Didn’t suspect a thing.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 96 elements of type TokenWithId,\n",
       "          target: list with 161 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [161, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 161,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 161,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.237021,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Dog didn’t know it was his birthday. The dog didn’t know it had a birthday. The dog wasn’t sure why people were in the apartment. It was the dog’s third birthday, which in dog years doesn’t matter.Someone made that up and we just went along with it. Oh, one year equals seven for doggies?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Dog didn’t know it was his birthday. The dog didn’t know it had a birthday. The dog wasn’t sure why people were in the apartment. It was the dog’s third birthday, which in dog years doesn’t matter.Someone made that up and we just went along with it. Oh, one year equals seven for doggies?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"The dog didn’t know it was his birthday.\"\n",
       "  - \"The dog wasn’t sure why people were in the apartment.\"\n",
       "  - \"Someone made that up and we just went along with it.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 92 elements of type TokenWithId,\n",
       "          target: list with 157 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [157, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.224393,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Okay. When I see a dog, I’ll do math. That’s not fulfilling some dog need, you know? There’s not a dog sitting in a bar right now going, “I’m not three, I’m 21! I can legally drink!” That’s not how dogs keep track of time.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Okay. When I see a dog, I’ll do math. That’s not fulfilling some dog need, you know? There’s not a dog sitting in a bar right now going, “I’m not three, I’m 21! I can legally drink!” That’s not how dogs keep track of time.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"When I see a dog, I’ll do math.\"\n",
       "  \n",
       "  \n",
       "  - \"That’s not fulfilling some dog need, you know?\"\n",
       "  \n",
       "  \n",
       "  - \"There’s not a dog sitting in a bar right now'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 100 elements of type TokenWithId,\n",
       "          target: list with 165 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [165, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 100,\n",
       "          attr_pos_end: 165,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 100,\n",
       "          attr_pos_end: 165,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.497368,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If you have a dog, you know they don’t keep track of time. You’ve left your home, forgotten something, walked back in only to be greeted by your dog like you’ve just returned from war. “You’re back! It’s a miracle! You’re back after I don’t know how long ’cause I’m a dog.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If you have a dog, you know they don’t keep track of time. You’ve left your home, forgotten something, walked back in only to be greeted by your dog like you’ve just returned from war. “You’re back! It’s a miracle! You’re back after I don’t know how long ’cause I’m a dog.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You’re back! It’s a miracle! You’re back after I don’t know how long ’cause I’m a dog.\"\n",
       "  \n",
       "  \n",
       "  - \"You’re back! It’s'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Joe_List': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 94 elements of type TokenWithId,\n",
       "          target: list with 159 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [159, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 159,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 159,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.303377,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So I went to the ear, nose, throat doctor, which is a cool doctor. It’s one doctor. He knows all three body parts. I was hoping it was three doctors saving money, by sharing a small office. He’s like, “I’m Bill, this is Susan, and Ted. “Ear, nose, throat.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So I went to the ear, nose, throat doctor, which is a cool doctor. It’s one doctor. He knows all three body parts. I was hoping it was three doctors saving money, by sharing a small office. He’s like, “I’m Bill, this is Susan, and Ted. “Ear, nose, throat.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I was hoping it was three doctors saving money, by sharing a small office.\"\n",
       "  \n",
       "  \n",
       "  - \"He’s like, “I’m Bill, this is Susan, and Ted. “Ear, nose,'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 93 elements of type TokenWithId,\n",
       "          target: list with 158 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [158, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 158,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 158,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.099658,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''“Who do ya need?” And they all kinda. I went for my ear, I have an ear issue called, , tinnitus, or tin-uh-dus. That’s when you’re ears ring. You got it? Yeah, it’s frustrating. It’s when you’re ears ring, or buzz all the time.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''“Who do ya need?” And they all kinda. I went for my ear, I have an ear issue called, , tinnitus, or tin-uh-dus. That’s when you’re ears ring. You got it? Yeah, it’s frustrating. It’s when you’re ears ring, or buzz all the time.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I went for my ear, I have an ear issue called, , tinnitus, or tin-uh-dus.\"\n",
       "  - \"You got it? Yeah, it’s frustrating. It’s when you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 187 elements of type TokenWithId,\n",
       "          target: list with 252 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [252, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 187,\n",
       "          attr_pos_end: 252,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 187,\n",
       "          attr_pos_end: 252,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.723253,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know how you say it. I thought it was called tinnitus, but then I watched a YouTube video, and the doctor in the video, he kept saying “tin-uh-dus.” And so I was like, “Oh, I guess it’s pronounced tin-uh-dus.” But then I went to the comments section, and the first comment said, “This guy’s a fake ass fucking retard doctor. It’s pronounced t-ah-n-i-tus.” All caps. And then I was like, “Shit, now I don’t know how to say this word.” This gentlemen is a doctor, he seems very smart. This fella seems, less smart, but, extremely adamant about the pronunciation of this word. Tomato, to-mah-to.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t know how you say it. I thought it was called tinnitus, but then I watched a YouTube video, and the doctor in the video, he kept saying “tin-uh-dus.” And so I was like, “Oh, I guess it’s pronounced tin-uh-dus.” But then I went to the comments section, and the first comment said, “This guy’s a fake ass fucking retard doctor. It’s pronounced t-ah-n-i-tus.” All caps. And then I was like, “Shit, now I don’t know how to say this word.” This gentlemen is a doctor, he seems very smart. This fella seems, less smart, but, extremely adamant about the pronunciation of this word. Tomato, to-mah-to.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I thought it was called tinnitus, but then I watched a YouTube video, and the doctor in the video, he kept saying \"tin-uh-dus.\" And so I was like, \"Oh, I'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 160 elements of type TokenWithId,\n",
       "          target: list with 225 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [225, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 160,\n",
       "          attr_pos_end: 225,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 160,\n",
       "          attr_pos_end: 225,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.103299,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''However you say it. If you have it, don’t go to the doctor, they don’t do shit. I went to the doctor I was like “Doc, I have uh, tinnitus, or tin-uh-dus. “Whatever you guys are calling it. “My ears are ringing.” And then he said, “Okay, I’m gonna test your hearing.” And I was like, “Oh no, that’s okay. “My hearing is great. “I hear everything, plus ringing, so. “I kind of have superpower hearing, if you think about it. “I’m hearing shit that’s not even in the room.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''However you say it. If you have it, don’t go to the doctor, they don’t do shit. I went to the doctor I was like “Doc, I have uh, tinnitus, or tin-uh-dus. “Whatever you guys are calling it. “My ears are ringing.” And then he said, “Okay, I’m gonna test your hearing.” And I was like, “Oh no, that’s okay. “My hearing is great. “I hear everything, plus ringing, so. “I kind of have superpower hearing, if you think about it. “I’m hearing shit that’s not even in the room.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"If you have it, don’t go to the doctor, they don’t do shit.\"\n",
       "  \n",
       "  \n",
       "  - \"My ears are ringing.\"\n",
       "  \n",
       "  \n",
       "  - \"Oh no, that’s okay. \"My hearing'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Joe_List_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 126 elements of type TokenWithId,\n",
       "          target: list with 191 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [191, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 126,\n",
       "          attr_pos_end: 191,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 126,\n",
       "          attr_pos_end: 191,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.888041,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Back there. It doesn’t matter where are on the plane. You feel better than everybody behind you, don’t you? You can be in the second to last row. This one guy behind, you’re like what a fucking idiot back there. Embarrassing, loser. Last row. I was on a plane the other day. I hated the guy behind me. He kept yawning out loud, the whole flight. Am I a psycho or is that annoying? The whole flight he’s like.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Back there. It doesn’t matter where are on the plane. You feel better than everybody behind you, don’t you? You can be in the second to last row. This one guy behind, you’re like what a fucking idiot back there. Embarrassing, loser. Last row. I was on a plane the other day. I hated the guy behind me. He kept yawning out loud, the whole flight. Am I a psycho or is that annoying? The whole flight he’s like.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You can be in the second to last row. This one guy behind, you’re like what a fucking idiot back there.\"\n",
       "  \n",
       "  \n",
       "  - \"Am I a psycho or is that annoying? The whole flight he'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 171 elements of type TokenWithId,\n",
       "          target: list with 236 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [236, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 171,\n",
       "          attr_pos_end: 236,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 171,\n",
       "          attr_pos_end: 236,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.730421,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''[yawns] For like five hours. [yawns] I wanted him to die, I swear to God. I was like, hope this guy passes away on the flight. First of all, you don’t need to make a noise when you yawn. That’s a decision, he’s deciding to do that. It’s like, if you were hungry on a plane, you were like, I’m hungry. Well you’re all right? Yeah, yeah I’m hungry. I like to let people know when I’m hungry. I think it’s important for people to know that. People who yawn out loud, they want attention. That’s why they’re doing it.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''[yawns] For like five hours. [yawns] I wanted him to die, I swear to God. I was like, hope this guy passes away on the flight. First of all, you don’t need to make a noise when you yawn. That’s a decision, he’s deciding to do that. It’s like, if you were hungry on a plane, you were like, I’m hungry. Well you’re all right? Yeah, yeah I’m hungry. I like to let people know when I’m hungry. I think it’s important for people to know that. People who yawn out loud, they want attention. That’s why they’re doing it.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I was like, hope this guy passes away on the flight.\"\n",
       "  - \"You don’t need to make a noise when you yawn. That’s a decision, he’s deciding to do that.\"\n",
       "  - \"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 222 elements of type TokenWithId,\n",
       "          target: list with 287 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [287, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 222,\n",
       "          attr_pos_end: 287,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 222,\n",
       "          attr_pos_end: 287,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 31.381451,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They want you to be like, “Oh, my God, are you tired?” And they’re like, I am, here’s my entire life story. of how I came to be tired. Don’t fall for it, it’s a trap. Plus isn’t it fun to not ask somebody a question when they really want you to ask them a question? You ever do that when someone was like, “I had a wild night last night.” and then you’re like, “Neat.” And then you just walked away. That’s like one of my favorite things to do. Just tell me your shitty story. Don’t make me ask for it. I flew recently, I was at the airport, La Guardia airport, here in New York, that’s my home airport. I was walking through the terminal. They were playing “Welcome To The Jungle” by Guns N’ Roses at the airport. That’s a weird airport song.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They want you to be like, “Oh, my God, are you tired?” And they’re like, I am, here’s my entire life story. of how I came to be tired. Don’t fall for it, it’s a trap. Plus isn’t it fun to not ask somebody a question when they really want you to ask them a question? You ever do that when someone was like, “I had a wild night last night.” and then you’re like, “Neat.” And then you just walked away. That’s like one of my favorite things to do. Just tell me your shitty story. Don’t make me ask for it. I flew recently, I was at the airport, La Guardia airport, here in New York, that’s my home airport. I was walking through the terminal. They were playing “Welcome To The Jungle” by Guns N’ Roses at the airport. That’s a weird airport song.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It's a trap.\"\n",
       "  - \"I'm here's my entire life story.\"\n",
       "  - \"Don't fall for it, it's a trap.\"\n",
       "  - \"Neat.\"\n",
       "  '\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 151 elements of type TokenWithId,\n",
       "          target: list with 216 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [216, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 151,\n",
       "          attr_pos_end: 216,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 151,\n",
       "          attr_pos_end: 216,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.609004,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m a rock and roll guy. I love Guns N’ Roses, I’m cool. But it’s a little much at the airport. I’m walking to the terminal. I just hear like. Du du du du du du du, aah! I’m like hey, can we cool it down a little bit? It’s 7:30 AM. I’m eating a muffin, looking for an outlet. I don’t think we need to rock this hard. Also it’s a little unnerving to be boarding a flight. And hear, “You’re gonna die!”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m a rock and roll guy. I love Guns N’ Roses, I’m cool. But it’s a little much at the airport. I’m walking to the terminal. I just hear like. Du du du du du du du, aah! I’m like hey, can we cool it down a little bit? It’s 7:30 AM. I’m eating a muffin, looking for an outlet. I don’t think we need to rock this hard. Also it’s a little unnerving to be boarding a flight. And hear, “You’re gonna die!”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  * \"Du du du du du du du du, aah!\"\n",
       "  * \"Hey, can we cool it down a little bit?\"\n",
       "  * \"You’re gonna die!\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'John_Mulaney': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 123 elements of type TokenWithId,\n",
       "          target: list with 188 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [188, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 123,\n",
       "          attr_pos_end: 188,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 123,\n",
       "          attr_pos_end: 188,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.582429,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t mean to sound down on donating. [chuckles] It’s good to give to charities, you know. My wife and I just gave a bunch of stuff to Goodwill. We were moving apartments, we had a bunch of clothes and furniture, so we made a whole day out of it. We made these big piles of clothes, we put the piles into these big boxes, then we put the boxes into the back of my car, and then they stayed there for four months.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I don’t mean to sound down on donating. [chuckles] It’s good to give to charities, you know. My wife and I just gave a bunch of stuff to Goodwill. We were moving apartments, we had a bunch of clothes and furniture, so we made a whole day out of it. We made these big piles of clothes, we put the piles into these big boxes, then we put the boxes into the back of my car, and then they stayed there for four months.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"It’s good to give to charities, you know.\"\n",
       "  - \"My wife and I just gave a bunch of stuff to Goodwill.\"\n",
       "  - \"We made these big piles of clothes, we put the piles into these big'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 128 elements of type TokenWithId,\n",
       "          target: list with 183 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [183, 55, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 128,\n",
       "          attr_pos_end: 183,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 128,\n",
       "          attr_pos_end: 183,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 20.822438,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And then one day my wife said, “Hey, you took that stuff to Goodwill, right?” And I said, “Of course I did! On an unrelated note, I’m going to walk out the front door right now.” So then I had to speed to Goodwill really fast. It was charitable, but it was also fast and violent, because I was throwing boxes at people. The boxes were so heavy I couldn’t even say what was in them. I was like, “This one’s shirts.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And then one day my wife said, “Hey, you took that stuff to Goodwill, right?” And I said, “Of course I did! On an unrelated note, I’m going to walk out the front door right now.” So then I had to speed to Goodwill really fast. It was charitable, but it was also fast and violent, because I was throwing boxes at people. The boxes were so heavy I couldn’t even say what was in them. I was like, “This one’s shirts.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Of course I did! On an unrelated note, I’m going to walk out the front door right now.\"\n",
       "  \n",
       "  \n",
       "  - \"This one’s shirts.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 96 elements of type TokenWithId,\n",
       "          target: list with 144 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [144, 48, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 144,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 144,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 17.171055,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I got a bunch of shirts! Take ’em away!” The guy tried to give me a big receipt. He’s like, “Take this receipt for the clothing for your taxes.” How do I write that on my taxes? “Dear IRS, please deduct from my federal income tax one XXL Billabong T-shirt from youth. It was too big.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I got a bunch of shirts! Take ’em away!” The guy tried to give me a big receipt. He’s like, “Take this receipt for the clothing for your taxes.” How do I write that on my taxes? “Dear IRS, please deduct from my federal income tax one XXL Billabong T-shirt from youth. It was too big.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I got a bunch of shirts! Take ’em away!\"\n",
       "  \n",
       "  - \"Take this receipt for the clothing for your taxes.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 219 elements of type TokenWithId,\n",
       "          target: list with 284 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [284, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 219,\n",
       "          attr_pos_end: 284,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 219,\n",
       "          attr_pos_end: 284,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 31.339099,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My mom said it could be a sleep shirt. Please deduct this from my 2017 income.” That sleep shirt bullshit. “Well, if it’s too big you can just wear it as a sleep shirt.” No, I get that, Mom, but why don’t we just tell our relatives that I’m a four-year-old boy and I don’t wear a man’s XXL T-shirt? “Because we don’t say that when someone gives us a gift because that would not be polite.” Oh, I get it. So rather than violate these meaningless politeness rules, I’ll just go to bed in a smock like goddamn Ebenezer Scrooge. Why don’t you give me a candle for looking in the mirror and a floppy hat and I’ll tremble off to bed in my long Victorian nightgown? Was there ever even a ghost, Mother, or was the dead Victorian girl you saw just me all along?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My mom said it could be a sleep shirt. Please deduct this from my 2017 income.” That sleep shirt bullshit. “Well, if it’s too big you can just wear it as a sleep shirt.” No, I get that, Mom, but why don’t we just tell our relatives that I’m a four-year-old boy and I don’t wear a man’s XXL T-shirt? “Because we don’t say that when someone gives us a gift because that would not be polite.” Oh, I get it. So rather than violate these meaningless politeness rules, I’ll just go to bed in a smock like goddamn Ebenezer Scrooge. Why don’t you give me a candle for looking in the mirror and a floppy hat and I’ll tremble off to bed in my long Victorian nightgown? Was there ever even a ghost, Mother, or was the dead Victorian girl you saw just me all along?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"My mom said it could be a sleep shirt. Please deduct this from my 2017 income.\"\n",
       "  \n",
       "  \n",
       "  - \"Well, if it's too big you can just wear it as a sleep shirt'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'John_Mulaney_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 122 elements of type TokenWithId,\n",
       "          target: list with 187 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [187, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 122,\n",
       "          attr_pos_end: 187,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 122,\n",
       "          attr_pos_end: 187,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.789266,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The greatest assembly of them all, once a year, Stranger Danger. Yeah, the hottest ticket in town. The Bruno Mars of assemblies. You are gathered together as a school and you are told never to talk to an adult that you don’t know and you are told this by an adult that you don’t know. We had the same Stranger Danger speaker every year when I was a kid, his name was Detective JJ Bittenbinder. Go ahead and laugh. His name is ridiculous.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The greatest assembly of them all, once a year, Stranger Danger. Yeah, the hottest ticket in town. The Bruno Mars of assemblies. You are gathered together as a school and you are told never to talk to an adult that you don’t know and you are told this by an adult that you don’t know. We had the same Stranger Danger speaker every year when I was a kid, his name was Detective JJ Bittenbinder. Go ahead and laugh. His name is ridiculous.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"The greatest assembly of them all, once a year, Stranger Danger.\"\n",
       "  - \"The Bruno Mars of assemblies.\"\n",
       "  - \"We had the same Stranger Danger speaker every year when I was a kid, his name was Detective JJ'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 94 elements of type TokenWithId,\n",
       "          target: list with 159 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [159, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 159,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 94,\n",
       "          attr_pos_end: 159,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.351121,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That was his name. It was JJ Bittenbinder. He was from the Chicago Police Department. He was a child homicide expert and… -[audience is silent] -Oh, gee. [audience laughing] Very sorry, Radio City, did that make you uncomfortable? Well, guess what? You’re adults and he’s not even here.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That was his name. It was JJ Bittenbinder. He was from the Chicago Police Department. He was a child homicide expert and… -[audience is silent] -Oh, gee. [audience laughing] Very sorry, Radio City, did that make you uncomfortable? Well, guess what? You’re adults and he’s not even here.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"That was his name. It was JJ Bittenbinder. He was from the Chicago Police Department. He was a child homicide expert and… -[audience is silent] -Oh, gee. [audience laughing'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 132 elements of type TokenWithId,\n",
       "          target: list with 197 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [197, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 132,\n",
       "          attr_pos_end: 197,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 132,\n",
       "          attr_pos_end: 197,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.907946,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So try being seven years old and you’re sitting five feet away from him. He’s still got blood on his shoes. And he’s looking at you in the eye to tell you for the first time in your very young life that some adults find you incredibly attractive. [audience laughing] And they may just have to kill you over it. Okay, c’est la vie, go be kids, go have fun. Bittenbinder came every year. By the way, Detective JJ Bittenbinder wore three-piece suits.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So try being seven years old and you’re sitting five feet away from him. He’s still got blood on his shoes. And he’s looking at you in the eye to tell you for the first time in your very young life that some adults find you incredibly attractive. [audience laughing] And they may just have to kill you over it. Okay, c’est la vie, go be kids, go have fun. Bittenbinder came every year. By the way, Detective JJ Bittenbinder wore three-piece suits.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He’s still got blood on his shoes. And he’s looking at you in the eye to tell you for the first time in your very young life that some adults find you incredibly attractive. [audience'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 139 elements of type TokenWithId,\n",
       "          target: list with 204 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [204, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 139,\n",
       "          attr_pos_end: 204,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 139,\n",
       "          attr_pos_end: 204,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.629945,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He also wore a pocket watch. Two years in a row, he wore a cowboy hat. He also had a huge handlebar mustache. None of that matters, but it’s important to me that you know that. He did not look like his job description. He looked like he should be the conductor on a locomotive powered by confetti. But, instead, he made his living in murder. He was the weirdest goddamn person I ever saw in my entire life. He was a man most acquainted with misery. He could look at a child and guess the price of their coffin.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''He also wore a pocket watch. Two years in a row, he wore a cowboy hat. He also had a huge handlebar mustache. None of that matters, but it’s important to me that you know that. He did not look like his job description. He looked like he should be the conductor on a locomotive powered by confetti. But, instead, he made his living in murder. He was the weirdest goddamn person I ever saw in my entire life. He was a man most acquainted with misery. He could look at a child and guess the price of their coffin.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - He also wore a pocket watch.\n",
       "  - He did not look like his job description.\n",
       "  - He was a man most acquainted with misery.\n",
       "  - He could look at a child and guess the price of their'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Jimmy_Yang': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 91 elements of type TokenWithId,\n",
       "          target: list with 156 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [156, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 156,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 156,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.156308,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Asian people, we don’t need Tinder anymore. We just go to BTS concerts. That’s how we do that parking lot pimping.I’ve been dating a lot of tall girls lately, because it makes me look successful. No, no. I think tall women are beautiful. But some of them like to wear heels.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Asian people, we don’t need Tinder anymore. We just go to BTS concerts. That’s how we do that parking lot pimping.I’ve been dating a lot of tall girls lately, because it makes me look successful. No, no. I think tall women are beautiful. But some of them like to wear heels.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Asian people, we don’t need Tinder anymore. We just go to BTS concerts. That’s how we do that parking lot pimping.\"\n",
       "  \n",
       "  \n",
       "  - \"I’ve been dating a lot of tall'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 83 elements of type TokenWithId,\n",
       "          target: list with 148 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [148, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 83,\n",
       "          attr_pos_end: 148,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 83,\n",
       "          attr_pos_end: 148,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.938258,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s just disrespect. Like, you’re already five inches taller than me. Why the fuck are you wearing heels? She’s like, it makes my ass look better. I’m like, your ass is at my eye level right now. Neither of us look good, OK?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s just disrespect. Like, you’re already five inches taller than me. Why the fuck are you wearing heels? She’s like, it makes my ass look better. I’m like, your ass is at my eye level right now. Neither of us look good, OK?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"That’s just disrespect. Like, you’re already five inches taller than me. Why the fuck are you wearing heels? She’s like, it makes my ass look better. I’m like'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 151 elements of type TokenWithId,\n",
       "          target: list with 216 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [216, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 151,\n",
       "          attr_pos_end: 216,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 151,\n",
       "          attr_pos_end: 216,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 26.922706,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I look like a child, and you look like a child molester.Last time– the last time I took a tall girl to this concert– I don’t know if you guys know this, but apparently, tall people have fun at concerts. Are you guys aware of that? I’m 5′ 5″. I just go to concerts to smell other people’s armpits. I’m like what the fuck’s the point of this? She was having the time of her life, doing whatever tall people do at concerts, you know, jumping around, obstructing other people’s views, seeing everything.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I look like a child, and you look like a child molester.Last time– the last time I took a tall girl to this concert– I don’t know if you guys know this, but apparently, tall people have fun at concerts. Are you guys aware of that? I’m 5′ 5″. I just go to concerts to smell other people’s armpits. I’m like what the fuck’s the point of this? She was having the time of her life, doing whatever tall people do at concerts, you know, jumping around, obstructing other people’s views, seeing everything.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I look like a child, and you look like a child molester.\"\n",
       "  \n",
       "  \n",
       "  - \"Are you guys aware of that? I’m 5′ 5″. I just go to concerts to smell'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 72 elements of type TokenWithId,\n",
       "          target: list with 137 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [137, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 72,\n",
       "          attr_pos_end: 137,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 72,\n",
       "          attr_pos_end: 137,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.20211,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was frustrated. I had enough. So I just looked up at her, I was like, hey! Pick me up. This is bullshit. I paid for these tickets, OK? I want to see Billy Eilish, too. Come on.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was frustrated. I had enough. So I just looked up at her, I was like, hey! Pick me up. This is bullshit. I paid for these tickets, OK? I want to see Billy Eilish, too. Come on.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I was frustrated. I had enough. So I just looked up at her, I was like, hey! Pick me up. This is bullshit.\"\n",
       "  \n",
       "  \n",
       "  - \"I paid for these tickets, OK? I want to see Billy'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Jimmy_Yang_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 98 elements of type TokenWithId,\n",
       "          target: list with 163 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [163, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 98,\n",
       "          attr_pos_end: 163,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 98,\n",
       "          attr_pos_end: 163,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.457433,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''See, I’m like first generation. But my parents, they’re like negative 9 generation, because they’re so frickin’ Chinese. Like, it’s really hard for me to watch TV with my dad, because he’s trying to make me explain everything to him. First of all, old Asian people, they don’t watch TV.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''See, I’m like first generation. But my parents, they’re like negative 9 generation, because they’re so frickin’ Chinese. Like, it’s really hard for me to watch TV with my dad, because he’s trying to make me explain everything to him. First of all, old Asian people, they don’t watch TV.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s really hard for me to watch TV with my dad, because he’s trying to make me explain everything to him.\"\n",
       "  \n",
       "  \n",
       "  - \"First of all, old Asian people, they don’t'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 111 elements of type TokenWithId,\n",
       "          target: list with 176 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [176, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 111,\n",
       "          attr_pos_end: 176,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 111,\n",
       "          attr_pos_end: 176,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.201457,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They judge the TV. This is like, I’m just sitting next to my dad on the couch, and he’s wearing his, like, old Asian man costume, which is just a wife beater and tighty-whiteys. He’s just sitting there, arms folded, judging the TV like–He’s made some random noises around the house. Now whenever he sneezes, it’s never just a sneeze.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They judge the TV. This is like, I’m just sitting next to my dad on the couch, and he’s wearing his, like, old Asian man costume, which is just a wife beater and tighty-whiteys. He’s just sitting there, arms folded, judging the TV like–He’s made some random noises around the house. Now whenever he sneezes, it’s never just a sneeze.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He’s just sitting there, arms folded, judging the TV like–He’s made some random noises around the house.\"\n",
       "  \n",
       "  \n",
       "  - \"Now whenever he sneezes, it’s never just a sneeze'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 53 elements of type TokenWithId,\n",
       "          target: list with 117 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [117, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 53,\n",
       "          attr_pos_end: 117,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 53,\n",
       "          attr_pos_end: 117,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 20.5956,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s like a whole tsunami of sound waves that comes after. It’s just like, achoo! Oh! Ay, shit.Oh!'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s like a whole tsunami of sound waves that comes after. It’s just like, achoo! Oh! Ay, shit.Oh!'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"It’s like a whole tsunami of sound waves that comes after. It’s just like, achoo! Oh! Ay, shit.Oh!\"\n",
       "  \n",
       "  - \"Achoo! Oh! Ay, shit.Oh!\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 78 elements of type TokenWithId,\n",
       "          target: list with 140 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [140, 62, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 78,\n",
       "          attr_pos_end: 140,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 78,\n",
       "          attr_pos_end: 140,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.508457,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m like, what the fuck, Dad, just have an orgasm? What was that?And he doesn’t understand what I’m saying half the time. He’s like, oh, there’s an orgasm. OK, the orga– orgasm.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m like, what the fuck, Dad, just have an orgasm? What was that?And he doesn’t understand what I’m saying half the time. He’s like, oh, there’s an orgasm. OK, the orga– orgasm.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I’m like, what the fuck, Dad, just have an orgasm? What was that?\"\n",
       "  \n",
       "  - \"Oh, there’s an orgasm. OK, the orga– orgasm.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Louis_CK': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 111 elements of type TokenWithId,\n",
       "          target: list with 159 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [159, 48, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 111,\n",
       "          attr_pos_end: 159,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 111,\n",
       "          attr_pos_end: 159,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 17.529381,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was talking to my friend the other day about Jesus… uh, Christ, and, um… I don’t remember why, but I happened to mention that Jesus was Jewish and my friend said, “He was?” And I said, Yeah. Jesus was Jewish. And he said, “I don’t think so.” And I said, that’s okay, it already all happened. Doesn’t matter where you think.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was talking to my friend the other day about Jesus… uh, Christ, and, um… I don’t remember why, but I happened to mention that Jesus was Jewish and my friend said, “He was?” And I said, Yeah. Jesus was Jewish. And he said, “I don’t think so.” And I said, that’s okay, it already all happened. Doesn’t matter where you think.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He was?\"\n",
       "  - \"I don't think so.\"\n",
       "  - \"That's okay, it already happened.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 71 elements of type TokenWithId,\n",
       "          target: list with 123 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [123, 52, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 71,\n",
       "          attr_pos_end: 123,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 71,\n",
       "          attr_pos_end: 123,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 17.557043,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But he’d argued with me. He was like, “Dude, Jesus couldn’t be Jewish. Think about it.” I’m like, “You fucking think about it, you idiot. What d– What was he then?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But he’d argued with me. He was like, “Dude, Jesus couldn’t be Jewish. Think about it.” I’m like, “You fucking think about it, you idiot. What d– What was he then?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"Dude, Jesus couldn't be Jewish. Think about it.\"\n",
       "  - \"You fucking think about it, you idiot. What d– What was he then?\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 53 elements of type TokenWithId,\n",
       "          target: list with 110 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [110, 57, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 53,\n",
       "          attr_pos_end: 110,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 53,\n",
       "          attr_pos_end: 110,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 18.390436,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You’re… What, was he Presbyterian? What was he? Catholic? Okay, Jesus was Catholic and he had a gold chain with a cross.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You’re… What, was he Presbyterian? What was he? Catholic? Okay, Jesus was Catholic and he had a gold chain with a cross.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You’re… What, was he Presbyterian? What was he?\"\n",
       "  \n",
       "  - \"Catholic? Okay, Jesus was Catholic and he had a gold chain with a cross.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 75 elements of type TokenWithId,\n",
       "          target: list with 140 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [140, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 75,\n",
       "          attr_pos_end: 140,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 75,\n",
       "          attr_pos_end: 140,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.208047,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And when they nailed him up, he was like, “Oh, that’s why we have those!” “That finally makes sense. I didn’t even know. Oh, fuck, that’s me! I’m the little guy on it!”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And when they nailed him up, he was like, “Oh, that’s why we have those!” “That finally makes sense. I didn’t even know. Oh, fuck, that’s me! I’m the little guy on it!”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Oh, that’s why we have those!\"\n",
       "  - \"That finally makes sense. I didn’t even know.\"\n",
       "  - \"Oh, fuck, that’s me! I’m the little'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Louis_CK_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 58 elements of type TokenWithId,\n",
       "          target: list with 119 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [119, 61, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 58,\n",
       "          attr_pos_end: 119,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 58,\n",
       "          attr_pos_end: 119,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 19.927479,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Dogs are so dumb, it’s fucking tragic. It’s sad how dumb they are. They’re in our lives and they know nothing about what’s happening.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Dogs are so dumb, it’s fucking tragic. It’s sad how dumb they are. They’re in our lives and they know nothing about what’s happening.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"Dogs are so dumb, it’s fucking tragic.\"\n",
       "  - \"It’s sad how dumb they are.\"\n",
       "  - \"They’re in our lives and they know nothing about what’s happening.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 85 elements of type TokenWithId,\n",
       "          target: list with 150 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [150, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 150,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 150,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.958255,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You ever been having, like, a dramatic moment in your family, like, you’re in the living room telling the kids that grandma died, and everybody’s crying, and the dog’s sitting there like…“I know you! Ha!” They’re so stupid! Incredibly stupid animals.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You ever been having, like, a dramatic moment in your family, like, you’re in the living room telling the kids that grandma died, and everybody’s crying, and the dog’s sitting there like…“I know you! Ha!” They’re so stupid! Incredibly stupid animals.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You ever been having, like, a dramatic moment in your family, like, you’re in the living room telling the kids that grandma died, and everybody’s crying, and the dog’s sitting'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 108 elements of type TokenWithId,\n",
       "          target: list with 173 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [173, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 108,\n",
       "          attr_pos_end: 173,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 108,\n",
       "          attr_pos_end: 173,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.858993,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They don’t even know their own lives, they don’t even — they can’t even handle their own lives mentally. You ever throw a ball for your dog and your dog gets the ball and brings it, and then you throw the ball again, and the dog brings it. And then one of the times you throw the ball, the dog looks at you like, “I didn’t see what happened.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They don’t even know their own lives, they don’t even — they can’t even handle their own lives mentally. You ever throw a ball for your dog and your dog gets the ball and brings it, and then you throw the ball again, and the dog brings it. And then one of the times you throw the ball, the dog looks at you like, “I didn’t see what happened.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"They don’t even know their own lives, they don’t even — they can’t even handle their own lives mentally.\"\n",
       "  \n",
       "  \n",
       "  - \"You ever throw a ball for your dog and your dog gets'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 101 elements of type TokenWithId,\n",
       "          target: list with 158 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [158, 57, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 101,\n",
       "          attr_pos_end: 158,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 101,\n",
       "          attr_pos_end: 158,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 20.397123,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m sorry, I… I don’t know anything now. Please help.” So you point at the ball, “There it is, right there. It’s right there.” And the dog just looks at your finger ’cause there’s just no way he’s gonna get this concept that there’s an invisible line… …that he can follow with his imagination.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m sorry, I… I don’t know anything now. Please help.” So you point at the ball, “There it is, right there. It’s right there.” And the dog just looks at your finger ’cause there’s just no way he’s gonna get this concept that there’s an invisible line… …that he can follow with his imagination.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I’m sorry, I… I don’t know anything now. Please help.\"\n",
       "  \n",
       "  - \"There it is, right there. It’s right there.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Nate_Bargatze': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 147 elements of type TokenWithId,\n",
       "          target: list with 211 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [211, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 147,\n",
       "          attr_pos_end: 211,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 147,\n",
       "          attr_pos_end: 211,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.941172,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So I have a daughter, and my daughter’s name is Harper. So a lot of people ask, “Did you name her after Harper Lee, the author of To Kill a Mockingbird?” And, you know, I’ve never thought about an author a day in my life, so… That never occurred to me. I mean, my middle name is Lee, and it just never crossed my mind. Uh… I love having a kid. We… I love when kids cry, it’s just innocent. I love how innocent it is. They cry over a tag in their shirt.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So I have a daughter, and my daughter’s name is Harper. So a lot of people ask, “Did you name her after Harper Lee, the author of To Kill a Mockingbird?” And, you know, I’ve never thought about an author a day in my life, so… That never occurred to me. I mean, my middle name is Lee, and it just never crossed my mind. Uh… I love having a kid. We… I love when kids cry, it’s just innocent. I love how innocent it is. They cry over a tag in their shirt.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I’ve never thought about an author a day in my life, so… That never occurred to me.\"\n",
       "  \n",
       "  \n",
       "  - \"I love how innocent it is. They cry over a tag in their shirt.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 107 elements of type TokenWithId,\n",
       "          target: list with 166 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [166, 59, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 107,\n",
       "          attr_pos_end: 166,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 107,\n",
       "          attr_pos_end: 166,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.517531,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I mean, they bawl. They don’t like… It feels weird. And then, you could be like, “Is your house on fire?” I’ve never seen someone cry this much. It’s over nothing. She’s on her iPad a lot. You know, that’s the hard part. You got to get these kids off… You don’t want technology all the time.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I mean, they bawl. They don’t like… It feels weird. And then, you could be like, “Is your house on fire?” I’ve never seen someone cry this much. It’s over nothing. She’s on her iPad a lot. You know, that’s the hard part. You got to get these kids off… You don’t want technology all the time.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - They bawl.\n",
       "  - It feels weird.\n",
       "  - Is your house on fire?\n",
       "  - That's the hard part.\n",
       "  - You got to get these kids off.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 128 elements of type TokenWithId,\n",
       "          target: list with 193 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [193, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 128,\n",
       "          attr_pos_end: 193,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 128,\n",
       "          attr_pos_end: 193,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.202667,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She just sits there on her iPad. She wants to be a YouTuber, which, as a comedian, makes me furious. She watches these kid YouTube videos, and now she makes her own videos. It’s just her sitting there going, “Hey, guys, what’s going on?” “Click the links below, subscribe, leave a comment.” None of this is anywhere but my phone. I got 90 hours of this. It’s not on YouTube. Me and her mom are the only subscribers.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She just sits there on her iPad. She wants to be a YouTuber, which, as a comedian, makes me furious. She watches these kid YouTube videos, and now she makes her own videos. It’s just her sitting there going, “Hey, guys, what’s going on?” “Click the links below, subscribe, leave a comment.” None of this is anywhere but my phone. I got 90 hours of this. It’s not on YouTube. Me and her mom are the only subscribers.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"She wants to be a YouTuber, which, as a comedian, makes me furious.\"\n",
       "  \n",
       "  \n",
       "  - \"It’s just her sitting there going, “Hey, guys, what’s going on?” “Click'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 136 elements of type TokenWithId,\n",
       "          target: list with 201 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [201, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 136,\n",
       "          attr_pos_end: 201,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 136,\n",
       "          attr_pos_end: 201,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 25.321218,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She just watches kids playing with toys. Like, that’s what’s crazy. It’s not like a show. I’d be fine if she watched a show. She watches just a kid that’s like, “Hey, you don’t have this toy, I do.” “You want to see me open it?” It’s got five billion views. It looks like we buy her nothing, like, we just show her. “If you wanna watch a kid have fun, but you’ll never have fun in this house!”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''She just watches kids playing with toys. Like, that’s what’s crazy. It’s not like a show. I’d be fine if she watched a show. She watches just a kid that’s like, “Hey, you don’t have this toy, I do.” “You want to see me open it?” It’s got five billion views. It looks like we buy her nothing, like, we just show her. “If you wanna watch a kid have fun, but you’ll never have fun in this house!”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  * \"It’s not like a show. I’d be fine if she watched a show.\"\n",
       "  * \"You want to see me open it?\"\n",
       "  * \"If you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Nate_Bargatze_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 78 elements of type TokenWithId,\n",
       "          target: list with 125 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [125, 47, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 78,\n",
       "          attr_pos_end: 125,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 78,\n",
       "          attr_pos_end: 125,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 16.029477,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We’re doing homework too. Our daughter started bringing it home. Homework’s fun. First and second grade was awesome. Third grade, you’re like, “Okay.” They throw some stuff in, you’re like, “Oh, all right.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''We’re doing homework too. Our daughter started bringing it home. Homework’s fun. First and second grade was awesome. Third grade, you’re like, “Okay.” They throw some stuff in, you’re like, “Oh, all right.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"First and second grade was awesome.\"\n",
       "  - \"They throw some stuff in, you’re like, “Oh, all right.”\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 112 elements of type TokenWithId,\n",
       "          target: list with 173 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [173, 61, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 173,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 173,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.485382,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All right.” It’s, uh… “Okay, learning it earlier than we used to, huh?” I don’t even know if that’s true, but… She brought home Common Core math. That’s fun. It’s a new math they invented, no heads up. Just give it to parents that never learned it. Uh… It’s just a whole new… I mean, it’s unbelievable.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''All right.” It’s, uh… “Okay, learning it earlier than we used to, huh?” I don’t even know if that’s true, but… She brought home Common Core math. That’s fun. It’s a new math they invented, no heads up. Just give it to parents that never learned it. Uh… It’s just a whole new… I mean, it’s unbelievable.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"She brought home Common Core math.\"\n",
       "  - \"It’s a new math they invented, no heads up.\"\n",
       "  - \"It’s just a whole new… I mean, it’s unbelievable.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 102 elements of type TokenWithId,\n",
       "          target: list with 167 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [167, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 102,\n",
       "          attr_pos_end: 167,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 102,\n",
       "          attr_pos_end: 167,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.826245,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They bring it home, you gotta watch a 40-minute YouTube video on Common Core math. I don’t even understand it. If you know it… If you don’t know Common Core, it’s just a new math. And the goal of Common Core is to use one sheet of paper for every problem. You… You just want to keep breaking the problem down.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''They bring it home, you gotta watch a 40-minute YouTube video on Common Core math. I don’t even understand it. If you know it… If you don’t know Common Core, it’s just a new math. And the goal of Common Core is to use one sheet of paper for every problem. You… You just want to keep breaking the problem down.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"They bring it home, you gotta watch a 40-minute YouTube video on Common Core math.\"\n",
       "  \n",
       "  \n",
       "  - \"If you know it, you don’t even understand it.\"\n",
       "  \n",
       "  \n",
       "  - \"The goal'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 236 elements of type TokenWithId,\n",
       "          target: list with 301 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [301, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 236,\n",
       "          attr_pos_end: 301,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 236,\n",
       "          attr_pos_end: 301,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 32.851201,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You put the problem at the top, and it just keeps going. And then what’s even funnier is you see old math in the middle of it. As you break it down, old math gets in there and you’re like, “Oh! Just do that at the top. I don’t even know what we’re doing.” It’s not like old math isn’t working. Old math still… I don’t get incorrect change everywhere, just going, “This stupid old math!”\n",
       "  It’s a long way to get at the same answer. I told my wife, it feels like if you knocked on my front door and I opened it, and you say, “Can I come in?” And I was like, “Do you mind coming in through the back door?” “Does the front door not work?” “No, it works. I use it, a lot of people still use it, but the new way is to go jump the fence and come in the back and meet me at this same spot.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You put the problem at the top, and it just keeps going. And then what’s even funnier is you see old math in the middle of it. As you break it down, old math gets in there and you’re like, “Oh! Just do that at the top. I don’t even know what we’re doing.” It’s not like old math isn’t working. Old math still… I don’t get incorrect change everywhere, just going, “This stupid old math!”\n",
       "  It’s a long way to get at the same answer. I told my wife, it feels like if you knocked on my front door and I opened it, and you say, “Can I come in?” And I was like, “Do you mind coming in through the back door?” “Does the front door not work?” “No, it works. I use it, a lot of people still use it, but the new way is to go jump the fence and come in the back and meet me at this same spot.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s a long way to get at the same answer.\"\n",
       "  - \"Old math still… I don’t get incorrect change everywhere, just going, ‘This stupid old math!’\"\n",
       "  - \"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Nate_Bargatze_TK': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 87 elements of type TokenWithId,\n",
       "          target: list with 143 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [143, 56, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 143,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 143,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 19.705904,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My dad is a magician. He’s done that my entire life. He was a clown at the very beginning, just in case you’re like, “How do you get into something like that?” Uh… It goes clown, then magic. There’s two steps. You can take them in either order.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''My dad is a magician. He’s done that my entire life. He was a clown at the very beginning, just in case you’re like, “How do you get into something like that?” Uh… It goes clown, then magic. There’s two steps. You can take them in either order.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It goes clown, then magic.\"\n",
       "  - \"Uh… It goes clown, then magic.\"\n",
       "  - \"Two steps. You can take them in either order.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 107 elements of type TokenWithId,\n",
       "          target: list with 172 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [172, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 107,\n",
       "          attr_pos_end: 172,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 107,\n",
       "          attr_pos_end: 172,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.688274,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was born, he was a clown. It was never weird to me. I thought everybody’s dad was a clown. My, uh, first memory of my life is I was five years old, and I remember my mom walked me out to the front yard, and my dad pulled up… We had this old red Mazda. He’s dressed as a clown, that doesn’t even faze me.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I was born, he was a clown. It was never weird to me. I thought everybody’s dad was a clown. My, uh, first memory of my life is I was five years old, and I remember my mom walked me out to the front yard, and my dad pulled up… We had this old red Mazda. He’s dressed as a clown, that doesn’t even faze me.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"My, uh, first memory of my life is I was five years old, and I remember my mom walked me out to the front yard, and my dad pulled up… We had this old red Mazda.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 81 elements of type TokenWithId,\n",
       "          target: list with 146 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [146, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 81,\n",
       "          attr_pos_end: 146,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 81,\n",
       "          attr_pos_end: 146,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.686985,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s just how he left. How else would he come home? And… The Easter Bunny was in the passenger seat. That’s the first thing that I remember, to my life. If you want to know how you get into comedy, that’s a pretty good nudge.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s just how he left. How else would he come home? And… The Easter Bunny was in the passenger seat. That’s the first thing that I remember, to my life. If you want to know how you get into comedy, that’s a pretty good nudge.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  * \"That’s just how he left. How else would he come home? And… The Easter Bunny was in the passenger seat. That’s the first thing that I remember, to my life.\"\n",
       "  \n",
       "  \n",
       "  *'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 130 elements of type TokenWithId,\n",
       "          target: list with 194 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [194, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 130,\n",
       "          attr_pos_end: 194,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 130,\n",
       "          attr_pos_end: 194,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.713747,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And I remember it didn’t fit. The Easter Bunny head didn’t fit in the car. Like, he couldn’t sit normal, so his head was bent to the side. And I remember he had his seat belt on, and he’s just like… and I was like… I like to think about all the other people that saw that. Just in the car next to them, just at a red light, you’re like, “I didn’t even know they hung out like that.” Uh…'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And I remember it didn’t fit. The Easter Bunny head didn’t fit in the car. Like, he couldn’t sit normal, so his head was bent to the side. And I remember he had his seat belt on, and he’s just like… and I was like… I like to think about all the other people that saw that. Just in the car next to them, just at a red light, you’re like, “I didn’t even know they hung out like that.” Uh…'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - The Easter Bunny head didn’t fit in the car.\n",
       "  \n",
       "  \n",
       "  - He had his seat belt on, and he’s just like…\n",
       "  \n",
       "  \n",
       "  - I didn’t even know they hung out like that.'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Nate_Bargatze_TK_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 110 elements of type TokenWithId,\n",
       "          target: list with 164 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [164, 54, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 110,\n",
       "          attr_pos_end: 164,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 110,\n",
       "          attr_pos_end: 164,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 19.864239,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The best wedding I’ve ever been to was my cousin’s wedding. It was a real redneck affair. It was right outside Louisville, Kentucky, where my parents are from. And my cousin’s name is Tuesday, which is a good start. The invitation said, “Tuesday’s getting married, rehearsal’s Friday, wedding’s Saturday.” People were like, “We gotta be there Monday for this wedding?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''The best wedding I’ve ever been to was my cousin’s wedding. It was a real redneck affair. It was right outside Louisville, Kentucky, where my parents are from. And my cousin’s name is Tuesday, which is a good start. The invitation said, “Tuesday’s getting married, rehearsal’s Friday, wedding’s Saturday.” People were like, “We gotta be there Monday for this wedding?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"We gotta be there Monday for this wedding?\"\n",
       "  - \"Tuesday’s getting married, rehearsal’s Friday, wedding’s Saturday.\"\n",
       "  - \"Redneck affair.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 68 elements of type TokenWithId,\n",
       "          target: list with 122 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [122, 54, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 68,\n",
       "          attr_pos_end: 122,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 68,\n",
       "          attr_pos_end: 122,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 18.26434,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''How long is this wedding? It’s a week?” We get there, I’m wearing a button-down shirt, khaki pants. Nothing crazy. I’m wildly overdressed. I look like I work there.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''How long is this wedding? It’s a week?” We get there, I’m wearing a button-down shirt, khaki pants. Nothing crazy. I’m wildly overdressed. I look like I work there.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"It’s a week.”\n",
       "  - \"I’m wearing a button-down shirt, khaki pants. Nothing crazy.\"\n",
       "  - \"I look like I work there.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 93 elements of type TokenWithId,\n",
       "          target: list with 152 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [152, 59, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 152,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 93,\n",
       "          attr_pos_end: 152,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 21.007156,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''-Uh… Everybody else just has a football or basketball jersey on. My uncle, his daughter’s getting married. He has his tuxedo jacket, pants, cummerbund, bowtie. No shirt. They forgot his shirt, and instead of waiting to go get it, he was like, “Let’s do it without it.” Right?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''-Uh… Everybody else just has a football or basketball jersey on. My uncle, his daughter’s getting married. He has his tuxedo jacket, pants, cummerbund, bowtie. No shirt. They forgot his shirt, and instead of waiting to go get it, he was like, “Let’s do it without it.” Right?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"His daughter’s getting married. He has his tuxedo jacket, pants, cummerbund, bowtie. No shirt.\"\n",
       "  \n",
       "  \n",
       "  - \"Let’s do it without it.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 124 elements of type TokenWithId,\n",
       "          target: list with 189 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [189, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 124,\n",
       "          attr_pos_end: 189,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 124,\n",
       "          attr_pos_end: 189,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.563796,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like, that’s… And he doesn’t have a body that’s like, “That’s cool, man.” You know? He has a body that you’re like, “Put your jacket on backwards. Flip it around.” Yeah. Then you see his back, and you’re like, “Oof. All right, go back to regular way. Yeah, that’s my fault. I didn’t… How’s your back worse than your front?” Uh…'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Like, that’s… And he doesn’t have a body that’s like, “That’s cool, man.” You know? He has a body that you’re like, “Put your jacket on backwards. Flip it around.” Yeah. Then you see his back, and you’re like, “Oof. All right, go back to regular way. Yeah, that’s my fault. I didn’t… How’s your back worse than your front?” Uh…'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"You know? He has a body that you’re like, “Put your jacket on backwards. Flip it around.\" Yeah.\"\n",
       "  \n",
       "  \n",
       "  - \"Yeah, that’s my fault. I didn’t… How’s your'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Russell_Peters': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 97 elements of type TokenWithId,\n",
       "          target: list with 146 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [146, 49, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 146,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 97,\n",
       "          attr_pos_end: 146,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 17.612178,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So my doctor says, hey, what else is wrong with you? I go, what do you mean what else is wrong with me? He goes, look, you’re a 48-year-old Indian man. I’m like, that’s really fucking racist, but since you asked, I have acid reflux. I don’t know.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So my doctor says, hey, what else is wrong with you? I go, what do you mean what else is wrong with me? He goes, look, you’re a 48-year-old Indian man. I’m like, that’s really fucking racist, but since you asked, I have acid reflux. I don’t know.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"That’s really fucking racist, but since you asked, I have acid reflux.\"\n",
       "  \n",
       "  - \"I don't know.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 116 elements of type TokenWithId,\n",
       "          target: list with 181 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [181, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 116,\n",
       "          attr_pos_end: 181,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 116,\n",
       "          attr_pos_end: 181,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.55058,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Does anybody else– who else has acid reflux in here? First of all, you’re lying to me right now. Because there is no way you can be Indian and not have fucking acid reflux. It’s inevitable. There’s no way you can consume the food that we eat with that much spice, and that much oil, and that much butter, and not just have it burn a hole in your– as my dad would say– your esophagus.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Does anybody else– who else has acid reflux in here? First of all, you’re lying to me right now. Because there is no way you can be Indian and not have fucking acid reflux. It’s inevitable. There’s no way you can consume the food that we eat with that much spice, and that much oil, and that much butter, and not just have it burn a hole in your– as my dad would say– your esophagus.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"There's no way you can be Indian and not have fucking acid reflux.\"\n",
       "  \n",
       "  \n",
       "  - \"It's inevitable. There's no way you can consume the food that we eat with that much spice,'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 69 elements of type TokenWithId,\n",
       "          target: list with 134 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [134, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 134,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 69,\n",
       "          attr_pos_end: 134,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.116501,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''What? Son, it’s burning your esophagus. Dad, I want to assure you I have no phagus in me. No, no, son, esophagus. I don’t care whose phagus you think this is.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''What? Son, it’s burning your esophagus. Dad, I want to assure you I have no phagus in me. No, no, son, esophagus. I don’t care whose phagus you think this is.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"What? Son, it’s burning your esophagus. Dad, I want to assure you I have no phagus in me. No, no, son, esophagus. I don’t care whose phagus'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 103 elements of type TokenWithId,\n",
       "          target: list with 168 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [168, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 103,\n",
       "          attr_pos_end: 168,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 103,\n",
       "          attr_pos_end: 168,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.583901,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''There’s no way you don’t have acid reflux. I’m looking at all of you, especially all the pudgy guys. You know exactly who has it. You got it, don’t you, yellow guy? You do, do you. He’s like, I know, I have it, but I don’t want to say because my shirt looks like turmeric,'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''There’s no way you don’t have acid reflux. I’m looking at all of you, especially all the pudgy guys. You know exactly who has it. You got it, don’t you, yellow guy? You do, do you. He’s like, I know, I have it, but I don’t want to say because my shirt looks like turmeric,'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"There’s no way you don’t have acid reflux. I’m looking at all of you, especially all the pudgy guys. You know exactly who has it. You got it, don’'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Russell_Peters_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 89 elements of type TokenWithId,\n",
       "          target: list with 154 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [154, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 89,\n",
       "          attr_pos_end: 154,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 89,\n",
       "          attr_pos_end: 154,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.867583,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Identical twins are the only people that should be twins. And I started getting scared when I thought my girlfriend was going to have twins. But I started settling into the idea because I started thinking of names for identical twins. I was like, if I have twin girls– because you’ve got to have fun with the names.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Identical twins are the only people that should be twins. And I started getting scared when I thought my girlfriend was going to have twins. But I started settling into the idea because I started thinking of names for identical twins. I was like, if I have twin girls– because you’ve got to have fun with the names.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Identical twins are the only people that should be twins.\"\n",
       "  - \"I started getting scared when I thought my girlfriend was going to have twins.\"\n",
       "  - \"But I started settling into the idea because I'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 92 elements of type TokenWithId,\n",
       "          target: list with 157 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [157, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 92,\n",
       "          attr_pos_end: 157,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.244436,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If I have twin girls, these are going to be my daughters, it’s going to be Kate and Duplicate. These are my boys, it’s Pete and Repeat. If I had twins with a black girl, this is Tyrone and Tyclone. Identical twins, that’s like bragging rights for you, you know I mean.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''If I have twin girls, these are going to be my daughters, it’s going to be Kate and Duplicate. These are my boys, it’s Pete and Repeat. If I had twins with a black girl, this is Tyrone and Tyclone. Identical twins, that’s like bragging rights for you, you know I mean.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"If I have twin girls, these are going to be my daughters, it’s going to be Kate and Duplicate.\"\n",
       "  - \"These are my boys, it’s Pete and Repeat.\"\n",
       "  - \"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 60 elements of type TokenWithId,\n",
       "          target: list with 116 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [116, 56, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 60,\n",
       "          attr_pos_end: 116,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 60,\n",
       "          attr_pos_end: 116,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 18.474463,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s your way of going, look. Look at how good my balls are. Look, look. Or because you’re Punjabi, look at how good my junk they are.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That’s your way of going, look. Look at how good my balls are. Look, look. Or because you’re Punjabi, look at how good my junk they are.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"That’s your way of going, look. Look at how good my balls are.\"\n",
       "  - \"Or because you’re Punjabi, look at how good my junk they are.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 80 elements of type TokenWithId,\n",
       "          target: list with 145 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [145, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 145,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 80,\n",
       "          attr_pos_end: 145,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.531111,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Looks at this.Look at how good my balls are. My balls are so strong, they made one kid and then it made the exact same kid right away. You lift up your balls, sponsored by Xerox. Sponsored by Xerox. I call the left one copy and the right one paste'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Looks at this.Look at how good my balls are. My balls are so strong, they made one kid and then it made the exact same kid right away. You lift up your balls, sponsored by Xerox. Sponsored by Xerox. I call the left one copy and the right one paste'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"My balls are so strong, they made one kid and then it made the exact same kid right away.\"\n",
       "  \n",
       "  \n",
       "  - \"You lift up your balls, sponsored by Xerox. Sponsored by Xerox.\"\n",
       "  \n",
       "  \n",
       "  - \"I'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Sam_Morril': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 217 elements of type TokenWithId,\n",
       "          target: list with 282 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [282, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 217,\n",
       "          attr_pos_end: 282,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 217,\n",
       "          attr_pos_end: 282,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 31.061459,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I got roofied at a party in college by accident, but you know, it still counts. Obviously, it was an accident. No one at the party was like I’m gonna roofie that hairy boy over there. Let me a passed-out werewolf tonight. That is male privilege right there, isn’t it? I just told a room full of people I got roofied. I did not see one concerned face in here. Everyone in here is like I assume things worked out for you, and they did. Still weird, though. Here’s what happened. I was at a party and this guy handed a girl a drink, and she said, “I’m too drunk.” And I said, I’ll drink it. And he gave me this hateful look, and I thought, why is this guy so mad at me? And then I chugged it, and I woke up the next morning, and I was like oh.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I got roofied at a party in college by accident, but you know, it still counts. Obviously, it was an accident. No one at the party was like I’m gonna roofie that hairy boy over there. Let me a passed-out werewolf tonight. That is male privilege right there, isn’t it? I just told a room full of people I got roofied. I did not see one concerned face in here. Everyone in here is like I assume things worked out for you, and they did. Still weird, though. Here’s what happened. I was at a party and this guy handed a girl a drink, and she said, “I’m too drunk.” And I said, I’ll drink it. And he gave me this hateful look, and I thought, why is this guy so mad at me? And then I chugged it, and I woke up the next morning, and I was like oh.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I got roofied at a party in college by accident, but you know, it still counts.\"\n",
       "  \n",
       "  \n",
       "  - \"That is male privilege right there, isn’t it? I just told a room full of people I got roof'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 220 elements of type TokenWithId,\n",
       "          target: list with 285 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [285, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 220,\n",
       "          attr_pos_end: 285,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 220,\n",
       "          attr_pos_end: 285,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 31.31145,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That guy’s a sexual predator. And my friend said you ruined his night. And I was like, that’s not how I want to think about it, you know? You don’t want to think of yourself as a -block to some monster. I prefer to think of myself as an accidental hero. I have a negative-one rapes, so, you know. Statistically, you know… I can’t, you guys. I can’t take all the credit, you know? It’s kind of like if I was driving drunk and I just swerved and accidentally hit a mugger. And some woman is like, “My hero,” and I’m like if you say so, but I might have a substance abuse problem, so…\n",
       "  \n",
       "  It’s hard not to drink man. It’s very hard. I work in a place where there’s a bar every night. I’ve been trying to be good.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''That guy’s a sexual predator. And my friend said you ruined his night. And I was like, that’s not how I want to think about it, you know? You don’t want to think of yourself as a -block to some monster. I prefer to think of myself as an accidental hero. I have a negative-one rapes, so, you know. Statistically, you know… I can’t, you guys. I can’t take all the credit, you know? It’s kind of like if I was driving drunk and I just swerved and accidentally hit a mugger. And some woman is like, “My hero,” and I’m like if you say so, but I might have a substance abuse problem, so…\n",
       "  \n",
       "  It’s hard not to drink man. It’s very hard. I work in a place where there’s a bar every night. I’ve been trying to be good.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"That guy’s a sexual predator.\"\n",
       "  - \"You ruined his night.\"\n",
       "  - \"I prefer to think of myself as an accidental hero. I have a negative-one rapes, so, you'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 193 elements of type TokenWithId,\n",
       "          target: list with 258 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [258, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 193,\n",
       "          attr_pos_end: 258,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 193,\n",
       "          attr_pos_end: 258,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 29.598653,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s very difficult. I read this article that said try to replace drinking with another activity like tennis with a friend. I was like, yeah, that’s a very helpful example. That’s always the crossroads I find myself at at 4:57 a.m. I can’t tell you how often I’m coming out of a blackout like I should have done doubles instead. That’s… I brought my Wilson racket and everything. You tell me to replace an addiction with cardio, that’s very helpful. I’m gonna start walking into halfway houses full of crack heads like have you guys done Zumba? It’s so much better than crack. They’re like, thanks, we didn’t know. That was helpful. Of course you want to get fucked up. Have you tried living?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s very difficult. I read this article that said try to replace drinking with another activity like tennis with a friend. I was like, yeah, that’s a very helpful example. That’s always the crossroads I find myself at at 4:57 a.m. I can’t tell you how often I’m coming out of a blackout like I should have done doubles instead. That’s… I brought my Wilson racket and everything. You tell me to replace an addiction with cardio, that’s very helpful. I’m gonna start walking into halfway houses full of crack heads like have you guys done Zumba? It’s so much better than crack. They’re like, thanks, we didn’t know. That was helpful. Of course you want to get fucked up. Have you tried living?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"That’s always the crossroads I find myself at at 4:57 a.m. I can’t tell you how often I’m coming out of a blackout like I should have done doubles instead.\"\n",
       "  \n",
       "  \n",
       "  -'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 163 elements of type TokenWithId,\n",
       "          target: list with 228 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [228, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 163,\n",
       "          attr_pos_end: 228,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 163,\n",
       "          attr_pos_end: 228,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.472545,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m happy and I want to get fucked up. Have you watched the news? It makes you want to drink. Another mass shooting after another mass shooting. It’s always the same thing, too. It’s always some guy who’s like I heard voices in my head. We all do. Everyone hears voices in their head. Good people just don’t listen to them. If I listened to all the voices in my head, I would be a sometimes-gay street fighter, so… I know. It sounds like a confusing video game. It’s not, you know? Why is E. Honda butt-fucking me? Wow. That’s his finisher.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m happy and I want to get fucked up. Have you watched the news? It makes you want to drink. Another mass shooting after another mass shooting. It’s always the same thing, too. It’s always some guy who’s like I heard voices in my head. We all do. Everyone hears voices in their head. Good people just don’t listen to them. If I listened to all the voices in my head, I would be a sometimes-gay street fighter, so… I know. It sounds like a confusing video game. It’s not, you know? Why is E. Honda butt-fucking me? Wow. That’s his finisher.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It makes you want to drink. Another mass shooting after another mass shooting. It’s always the same thing, too. It’s always some guy who’s like I heard voices in my head.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Sam_Morril_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 164 elements of type TokenWithId,\n",
       "          target: list with 229 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [229, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 164,\n",
       "          attr_pos_end: 229,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 164,\n",
       "          attr_pos_end: 229,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.340598,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I did nothing all day, man. I did nothing. I watched the news. I saw an anti-smoking ad. It’s weird that they can do anti-smoking ads, but you can’t do pro-cigarette commercials. Isn’t that weird? They don’t give ’em a rebuttal. And I’m not like a big cigarette guy, but I like one when I’m drunk sometimes. It’s nice. I thought of a good pro-cigarette commercial. How ’bout this for like a casual smoker? A hot girl walks up to a guy in the bar and asks to bum a cigarette, and he goes, oh, I don’t smoke.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I did nothing all day, man. I did nothing. I watched the news. I saw an anti-smoking ad. It’s weird that they can do anti-smoking ads, but you can’t do pro-cigarette commercials. Isn’t that weird? They don’t give ’em a rebuttal. And I’m not like a big cigarette guy, but I like one when I’m drunk sometimes. It’s nice. I thought of a good pro-cigarette commercial. How ’bout this for like a casual smoker? A hot girl walks up to a guy in the bar and asks to bum a cigarette, and he goes, oh, I don’t smoke.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I did nothing all day, man. I did nothing.\"\n",
       "  - \"It’s weird that they can do anti-smoking ads, but you can’t do pro-cigarette commercials.\"\n",
       "  - \"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 194 elements of type TokenWithId,\n",
       "          target: list with 259 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [259, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 194,\n",
       "          attr_pos_end: 259,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 194,\n",
       "          attr_pos_end: 259,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 29.493166,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So then she goes outside and bums them from another guy, and they go home together and they fuck and that’s the whole commercial. There at the end, it just says, “Wouldn’t kill you to carry a pack,” you know? Marlboro: Just in Case. All right, I got some momentum. Let’s break out the abortion jokes. If there was ever a time. My friend just had one, and her boyfriend didn’t contribute to the cost, which I thought was not cool, you know? She’s going through it, yeah. That’s not a very noble stance, I don’t think. That is how low the bar is. I’m like, guy should pay a little bit for the abortion, and women are like, he’s a hero. He really is.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''So then she goes outside and bums them from another guy, and they go home together and they fuck and that’s the whole commercial. There at the end, it just says, “Wouldn’t kill you to carry a pack,” you know? Marlboro: Just in Case. All right, I got some momentum. Let’s break out the abortion jokes. If there was ever a time. My friend just had one, and her boyfriend didn’t contribute to the cost, which I thought was not cool, you know? She’s going through it, yeah. That’s not a very noble stance, I don’t think. That is how low the bar is. I’m like, guy should pay a little bit for the abortion, and women are like, he’s a hero. He really is.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"So then she goes outside and bums them from another guy, and they go home together and they fuck and that’s the whole commercial.\"\n",
       "  \n",
       "  \n",
       "  - \"That’s not a very noble stance, I don’t think'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 173 elements of type TokenWithId,\n",
       "          target: list with 238 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [238, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 173,\n",
       "          attr_pos_end: 238,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 173,\n",
       "          attr_pos_end: 238,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.000418,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''No, even the most pro-life person I talk to was like, wow, chivalry is dead. That was the baby’s name, Chivalry. I’m just saying. That one’s not for everybody, that joke. I did that joke the other night and somebody yelled out boo. I thought it was the baby’s ghost, but, you know. I’m just saying there are still gentlemen. If I knock you up and you need an abortion, it’s on the house. I’ll throw my Delta SkyMiles card right on the counter. Are you sure, Sam? I got this. With all the rewards and benefits that Delta has to offer, I’d be a fool to do otherwise.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''No, even the most pro-life person I talk to was like, wow, chivalry is dead. That was the baby’s name, Chivalry. I’m just saying. That one’s not for everybody, that joke. I did that joke the other night and somebody yelled out boo. I thought it was the baby’s ghost, but, you know. I’m just saying there are still gentlemen. If I knock you up and you need an abortion, it’s on the house. I’ll throw my Delta SkyMiles card right on the counter. Are you sure, Sam? I got this. With all the rewards and benefits that Delta has to offer, I’d be a fool to do otherwise.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"That one’s not for everybody, that joke.\"\n",
       "  - \"I’ll throw my Delta SkyMiles card right on the counter.\"\n",
       "  - \"Are you sure, Sam? I got this. With all the rewards and'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 247 elements of type TokenWithId,\n",
       "          target: list with 312 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [312, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 247,\n",
       "          attr_pos_end: 312,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 247,\n",
       "          attr_pos_end: 312,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 33.419818,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m two to three abortions away from Diamond Medallion status, so… I’m losing some of you, but I’m gonna take it a step further here. I think you should get extra SkyMiles points with an abortion, because that’s one less crying baby onboard, you know? That, yeah, I don’t… Me personally, I don’t like doing that part of the joke, but I have a sponsorship deal with Delta, and you know, they get very upset when I leave it out. They dock me Biscoff cookies. Love those cookies. I was talking to a guy after a show. He told me there was gonna be an anti-abortion parade, which I think he meant to say march, you know? I don’t think there’s gonna be a parade, but if there is, I’ll be there. I want to see what the floats look like. That one looks unfinished. That’s what we’re going for, so, yeah. Back-to-back abortion jokes. I got greedy, guys, I did.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I’m two to three abortions away from Diamond Medallion status, so… I’m losing some of you, but I’m gonna take it a step further here. I think you should get extra SkyMiles points with an abortion, because that’s one less crying baby onboard, you know? That, yeah, I don’t… Me personally, I don’t like doing that part of the joke, but I have a sponsorship deal with Delta, and you know, they get very upset when I leave it out. They dock me Biscoff cookies. Love those cookies. I was talking to a guy after a show. He told me there was gonna be an anti-abortion parade, which I think he meant to say march, you know? I don’t think there’s gonna be a parade, but if there is, I’ll be there. I want to see what the floats look like. That one looks unfinished. That’s what we’re going for, so, yeah. Back-to-back abortion jokes. I got greedy, guys, I did.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I’m two to three abortions away from Diamond Medallion status, so… I’m losing some of you, but I’m gonna take it a step further here. I think you should get extra'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Trevor_Noah': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 170 elements of type TokenWithId,\n",
       "          target: list with 235 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [235, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 170,\n",
       "          attr_pos_end: 235,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 170,\n",
       "          attr_pos_end: 235,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 27.732004,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You don’t mess with the Russians. Most frightening people in the world. You know how I know this? You know how I’ve learned? Because I’ve learned how to use the Russian accent for myself. I’ve learned how to harness that energy and use it for good. I’ll share this with you, I don’t mind. I, uh– I’m not particularly comfortable in the house at night by myself. What I’m trying to say is I’m afraid of the dark. And I know it’s stupid to be afraid of the dark, right? Because there’s nothing there, nine times out of ten. But what happens is, I’ll be sleeping.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''You don’t mess with the Russians. Most frightening people in the world. You know how I know this? You know how I’ve learned? Because I’ve learned how to use the Russian accent for myself. I’ve learned how to harness that energy and use it for good. I’ll share this with you, I don’t mind. I, uh– I’m not particularly comfortable in the house at night by myself. What I’m trying to say is I’m afraid of the dark. And I know it’s stupid to be afraid of the dark, right? Because there’s nothing there, nine times out of ten. But what happens is, I’ll be sleeping.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"You don’t mess with the Russians. Most frightening people in the world.\"\n",
       "  \n",
       "  \n",
       "  - \"I’m not particularly comfortable in the house at night by myself. What I’m trying to say is I’m afraid of'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 186 elements of type TokenWithId,\n",
       "          target: list with 251 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [251, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 186,\n",
       "          attr_pos_end: 251,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 186,\n",
       "          attr_pos_end: 251,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 28.85535,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And in the middle of the night, I’ll be woken up by the need to pee. And whenever that happens, I’m always faced with the eternal dilemma. When I go to the bathroom, do I turn the lights on and lose my sleep? Or do I leave the lights off and shit myself? I never know which one to go with. But what I started doing now is I leave the lights off. And when I go the bathroom, I just speak to myself in a Russian accent. It sounds crazy, but I feel safe. I feel like I’m the most dangerous thing in the night. You’ll find me at three a.m., barefoot, walking to the toilet, like, “Yes. Big boy got to make a pee-pee. No trouble over here.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And in the middle of the night, I’ll be woken up by the need to pee. And whenever that happens, I’m always faced with the eternal dilemma. When I go to the bathroom, do I turn the lights on and lose my sleep? Or do I leave the lights off and shit myself? I never know which one to go with. But what I started doing now is I leave the lights off. And when I go the bathroom, I just speak to myself in a Russian accent. It sounds crazy, but I feel safe. I feel like I’m the most dangerous thing in the night. You’ll find me at three a.m., barefoot, walking to the toilet, like, “Yes. Big boy got to make a pee-pee. No trouble over here.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"When I go to the bathroom, do I turn the lights on and lose my sleep? Or do I leave the lights off and shit myself?\"\n",
       "  \n",
       "  \n",
       "  - \"I never know which one to go with. But'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 214 elements of type TokenWithId,\n",
       "          target: list with 279 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [279, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 214,\n",
       "          attr_pos_end: 279,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 214,\n",
       "          attr_pos_end: 279,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 30.730112,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s potty time.”\n",
       "  \n",
       "  I feel safe, like even if there’s a monster under the bed, he’d be like, “Is that a Russian?” There’s just something about that Russian accent. Ladies, every single one of you needs to learn the Russian accent. You read stories all the time. I talk to my female friends all the time, my family members, and women are constantly under assault. Women are living a life of being vulnerable. Walking through the streets, men catcalling. In the office place, people groping. Feeling like they have ownership of the female form. I know we can’t solve this tomorrow, but if you learn the Russian accent, half of those problems would disappear. The next time you’re in a compromising position, you’re at a bar, waiting for your drink. That creepy guy comes up and starts grinding on you from behind. “Hey, how are you?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s potty time.”\n",
       "  \n",
       "  I feel safe, like even if there’s a monster under the bed, he’d be like, “Is that a Russian?” There’s just something about that Russian accent. Ladies, every single one of you needs to learn the Russian accent. You read stories all the time. I talk to my female friends all the time, my family members, and women are constantly under assault. Women are living a life of being vulnerable. Walking through the streets, men catcalling. In the office place, people groping. Feeling like they have ownership of the female form. I know we can’t solve this tomorrow, but if you learn the Russian accent, half of those problems would disappear. The next time you’re in a compromising position, you’re at a bar, waiting for your drink. That creepy guy comes up and starts grinding on you from behind. “Hey, how are you?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"It’s potty time.\"\n",
       "  - \"I know we can’t solve this tomorrow, but if you learn the Russian accent, half of those problems would disappear.\"\n",
       "  - \"That creepy guy comes up and starts grinding on'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 207 elements of type TokenWithId,\n",
       "          target: list with 272 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [272, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 207,\n",
       "          attr_pos_end: 272,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 207,\n",
       "          attr_pos_end: 272,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 30.44409,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Hey, you want to dance?” “No, I’m fine, thank you. I’m just getting a drink. I don’t–” “Come on, dance with me, girl.” “I’m fine. I don’t wanna dance. Thank you. I’m waiting for my friends. I’m good.” “Come on. ♪ I know you want me ♪\n",
       "  ♪ You know I want you ♪\n",
       "  \n",
       "  Come on! ♪ I know you want me ♪”\n",
       "  \n",
       "  “I don’t want dance with you. Please!” “Hey, you don’t gotta be such a bitch!” Whenever that happens, ladies, don’t be afraid, don’t stress. Just whip out your Russian and kill it dead. As soon as he says, “Don’t be such a bitch!” Just be like… [Russian accent] “You want to see bitch?”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Hey, you want to dance?” “No, I’m fine, thank you. I’m just getting a drink. I don’t–” “Come on, dance with me, girl.” “I’m fine. I don’t wanna dance. Thank you. I’m waiting for my friends. I’m good.” “Come on. ♪ I know you want me ♪\n",
       "  ♪ You know I want you ♪\n",
       "  \n",
       "  Come on! ♪ I know you want me ♪”\n",
       "  \n",
       "  “I don’t want dance with you. Please!” “Hey, you don’t gotta be such a bitch!” Whenever that happens, ladies, don’t be afraid, don’t stress. Just whip out your Russian and kill it dead. As soon as he says, “Don’t be such a bitch!” Just be like… [Russian accent] “You want to see bitch?”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  **Key Humorous Lines:**\n",
       "  \n",
       "  * \"Come on, dance with me, girl.\"\n",
       "  * \"I don't wanna dance. Thank you.\"\n",
       "  * \"I know you want me ♪\"\n",
       "  * \"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Trevor_Noah_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 85 elements of type TokenWithId,\n",
       "          target: list with 132 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [132, 47, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 132,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 85,\n",
       "          attr_pos_end: 132,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 16.300037,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Nelson Mandela was destined for greatness because of that voice. He could not be a normal man with a voice like that. You can’t do normal things with that voice. You are destined for greatness. You can’t be running in the streets: “And a Kit Kat, please.” No. No.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Nelson Mandela was destined for greatness because of that voice. He could not be a normal man with a voice like that. You can’t do normal things with that voice. You are destined for greatness. You can’t be running in the streets: “And a Kit Kat, please.” No. No.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"You can’t do normal things with that voice. You are destined for greatness.\"\n",
       "  - \"No. No.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 96 elements of type TokenWithId,\n",
       "          target: list with 161 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [161, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 161,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 96,\n",
       "          attr_pos_end: 161,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 23.450394,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s so unique. You can’t be silly. What are you making, prank phone calls? “Who the hell is this? !” “I’m not telling you.” [laughs] It just doesn’t work. I remember when Nelson Mandela was still alive, and he would tell jokes at press conferences and events, and no one would laugh.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It’s so unique. You can’t be silly. What are you making, prank phone calls? “Who the hell is this? !” “I’m not telling you.” [laughs] It just doesn’t work. I remember when Nelson Mandela was still alive, and he would tell jokes at press conferences and events, and no one would laugh.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It’s so unique. You can’t be silly. What are you making, prank phone calls? “Who the hell is this? !” “I’m not telling you.” [laughs]\"\n",
       "  \n",
       "  \n",
       "  '\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 112 elements of type TokenWithId,\n",
       "          target: list with 177 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [177, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 177,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 112,\n",
       "          attr_pos_end: 177,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.00411,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Because everyone thought a man who had been in jail for 27 years couldn’t make a joke. And yet, he still did. He still kept what was him. It was partly because of that voice. He would tell the joke, and it would just sound too epic for people to laugh. Everything he did, he’d just be like, “Ah, knock, knock.” People would be like, “Let him in.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Because everyone thought a man who had been in jail for 27 years couldn’t make a joke. And yet, he still did. He still kept what was him. It was partly because of that voice. He would tell the joke, and it would just sound too epic for people to laugh. Everything he did, he’d just be like, “Ah, knock, knock.” People would be like, “Let him in.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"He would tell the joke, and it would just sound too epic for people to laugh.\"\n",
       "  \n",
       "  \n",
       "  - \"Everything he did, he’d just be like, “Ah, knock, knock.” People would be'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 91 elements of type TokenWithId,\n",
       "          target: list with 155 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [155, 64, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 155,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 91,\n",
       "          attr_pos_end: 155,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.849023,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Let him in!” “No. No, no. You must say, ‘Who’s there? '” “He is so right. We’ve always got to ask, ‘Who’s there?’ Who’s here? Who are we?’ Oh, my God!” It was because of that voice, the first black president voice'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Let him in!” “No. No, no. You must say, ‘Who’s there? '” “He is so right. We’ve always got to ask, ‘Who’s there?’ Who’s here? Who are we?’ Oh, my God!” It was because of that voice, the first black president voice'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Let him in!\"\n",
       "  - \"No. No, no. You must say, 'Who's there? '\"\n",
       "  - \"He is so right. We've always got to ask, ''\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Tom_Segura': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 54 elements of type TokenWithId,\n",
       "          target: list with 103 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [103, 49, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 54,\n",
       "          attr_pos_end: 103,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 54,\n",
       "          attr_pos_end: 103,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 15.949438,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I bought weed from a dude in a stand-alone trailer one time. Not a trailer park. A solo trailer. The most terrifying housing situation that exists.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''I bought weed from a dude in a stand-alone trailer one time. Not a trailer park. A solo trailer. The most terrifying housing situation that exists.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"I bought weed from a dude in a stand-alone trailer one time.\"\n",
       "  - \"A solo trailer. The most terrifying housing situation that exists.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 95 elements of type TokenWithId,\n",
       "          target: list with 143 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [143, 48, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 95,\n",
       "          attr_pos_end: 143,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 95,\n",
       "          attr_pos_end: 143,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 17.034536,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Where other trailer people are like, “Get the fuck out of here.” Kick ’em out. I just walked up to that shit, 15. This dude’s like, “You trying to get a sack?” -I was like, “Oh, shit. Yeah.” “We could go do that.” I was like, “All right.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Where other trailer people are like, “Get the fuck out of here.” Kick ’em out. I just walked up to that shit, 15. This dude’s like, “You trying to get a sack?” -I was like, “Oh, shit. Yeah.” “We could go do that.” I was like, “All right.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Get the fuck out of here.\"\n",
       "  - \"We could go do that.\"\n",
       "  - \"Oh, shit. Yeah.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 131 elements of type TokenWithId,\n",
       "          target: list with 196 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [196, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 131,\n",
       "          attr_pos_end: 196,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 131,\n",
       "          attr_pos_end: 196,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.992151,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Cool.” And he goes, “We just need to go get it.” I was like, “You don’t fuckin’ have it? Isn’t that your sole responsibility?” I tried to play cool, “Let’s go get it.” He goes, “I’ll go get it. You stay here and watch my place.” And I was like… “Okay.” Then he goes, “There’s a .357 and a shotgun on my bed. Anybody comes in here, blast ’em.” Inside?'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Cool.” And he goes, “We just need to go get it.” I was like, “You don’t fuckin’ have it? Isn’t that your sole responsibility?” I tried to play cool, “Let’s go get it.” He goes, “I’ll go get it. You stay here and watch my place.” And I was like… “Okay.” Then he goes, “There’s a .357 and a shotgun on my bed. Anybody comes in here, blast ’em.” Inside?'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"We just need to go get it.\"\n",
       "  - \"I'll go get it. You stay here and watch my place.\"\n",
       "  - \"There's a .357 and a shotgun on my'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 90 elements of type TokenWithId,\n",
       "          target: list with 142 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [142, 52, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 90,\n",
       "          attr_pos_end: 142,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 90,\n",
       "          attr_pos_end: 142,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 18.277996,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Paralysis. But what I said was, “That’s what’s up.” -Like, yeah, man. Pow. Then he stopped at the door. “But don’t shoot my mom.” I go, “Can we get a description before we agree to terms? How about a height and weight on old mom?”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''Paralysis. But what I said was, “That’s what’s up.” -Like, yeah, man. Pow. Then he stopped at the door. “But don’t shoot my mom.” I go, “Can we get a description before we agree to terms? How about a height and weight on old mom?”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the transcript:\n",
       "  \n",
       "  - \"That's what's up.\"\n",
       "  - \"But don't shoot my mom.\"\n",
       "  - \"Can we get a description before we agree to terms?\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })],\n",
       " 'Tom_Segura_2': [FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 63 elements of type TokenWithId,\n",
       "          target: list with 111 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [111, 48, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 63,\n",
       "          attr_pos_end: 111,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 63,\n",
       "          attr_pos_end: 111,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 15.96966,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''By the way, is there any more satisfying feeling than letting an elevator door close on somebody? I did it… I did it at the hotel earlier. I got such a warm rush through my body.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''By the way, is there any more satisfying feeling than letting an elevator door close on somebody? I did it… I did it at the hotel earlier. I got such a warm rush through my body.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"I got such a warm rush through my body.\"\n",
       "  \n",
       "  - \"I did it… I did it at the hotel earlier.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 79 elements of type TokenWithId,\n",
       "          target: list with 144 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [144, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 79,\n",
       "          attr_pos_end: 144,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 79,\n",
       "          attr_pos_end: 144,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 22.574898,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It felt like the inside of my body hugged the outside of my body, you know? I was trying to figure out, “Why does this feel so good?” I think it’s a taste of power. Like most of us, we have no power in our everyday lives.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''It felt like the inside of my body hugged the outside of my body, you know? I was trying to figure out, “Why does this feel so good?” I think it’s a taste of power. Like most of us, we have no power in our everyday lives.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"It felt like the inside of my body hugged the outside of my body, you know? I was trying to figure out, “Why does this feel so good?”\n",
       "  \n",
       "  \n",
       "  - \"Like most of us, we'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 87 elements of type TokenWithId,\n",
       "          target: list with 144 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [144, 57, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 144,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 87,\n",
       "          attr_pos_end: 144,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 20.009126,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But if you’re alone in an elevator, -you are lord of the elevator shaft. You get to decide, like a king with his drawbridge. There’s “Hold Open,” and “Close.” And you can watch people walk up and be like, “Mm-mm.” -And you hit that.'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''But if you’re alone in an elevator, -you are lord of the elevator shaft. You get to decide, like a king with his drawbridge. There’s “Hold Open,” and “Close.” And you can watch people walk up and be like, “Mm-mm.” -And you hit that.'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"There's \"Hold Open,\" and \"Close.\" And you can watch people walk up and be like, \"Mm-mm.\"\n",
       "  \n",
       "  \n",
       "  - \"You hit that.\"'\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  }),\n",
       "  FeatureAttributionOutput({\n",
       "      sequence_attributions: list with 1 elements of type GradientFeatureAttributionSequenceOutput:[\n",
       "          GradientFeatureAttributionSequenceOutput({\n",
       "          source: list with 117 elements of type TokenWithId,\n",
       "          target: list with 182 elements of type TokenWithId,\n",
       "          source_attributions: None,\n",
       "          target_attributions: torch.float32 tensor of shape [182, 65, 2048] on cpu,\n",
       "          step_scores: {},\n",
       "          sequence_scores: None,\n",
       "          attr_pos_start: 117,\n",
       "          attr_pos_end: 182,\n",
       "      }\n",
       "      ],\n",
       "      step_attributions: None,\n",
       "      info: {\n",
       "          attribution_method: input_x_gradient,\n",
       "          attr_pos_start: 117,\n",
       "          attr_pos_end: 182,\n",
       "          output_step_attributions: False,\n",
       "          attribute_target: False,\n",
       "          step_scores: list with 0 elements,\n",
       "          exec_time: 24.42125,\n",
       "          model_name: google/gemma-2b-it,\n",
       "          model_class: GemmaForCausalLM,\n",
       "          tokenizer_name: None,\n",
       "          tokenizer_class: GemmaTokenizerFast,\n",
       "          include_eos_baseline: False,\n",
       "          attributed_fn: probability_fn,\n",
       "          attribution_args: {},\n",
       "          attributed_fn_args: {},\n",
       "          step_scores_args: {},\n",
       "          input_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And then you see it close, and you’re like… Sometimes, a second later it opens, and you’re like, “Fuck!” You get nervous energy, like you’re a kid. You’re like, “I’m in trouble.” It’s always some lady who’s like, “You didn’t see me?” “I don’t even know how this thing works. So many buttons. I tried all of them.”'''\n",
       "  Extract the key humorous lines and punchlines:'\n",
       "          ],\n",
       "          generated_texts: list with 1 elements of type str:[\n",
       "              'This is a stand-up comedy transcript: '''And then you see it close, and you’re like… Sometimes, a second later it opens, and you’re like, “Fuck!” You get nervous energy, like you’re a kid. You’re like, “I’m in trouble.” It’s always some lady who’s like, “You didn’t see me?” “I don’t even know how this thing works. So many buttons. I tried all of them.”'''\n",
       "  Extract the key humorous lines and punchlines:\n",
       "  \n",
       "  Sure, here are the key humorous lines and punchlines from the stand-up comedy transcript:\n",
       "  \n",
       "  - \"Sometimes, a second later it opens, and you’re like, ‘Fuck!’ You get nervous energy, like you’re a kid. You’re like, ‘I’m in trouble.’”\n",
       "  \n",
       "  \n",
       "  '\n",
       "          ],\n",
       "          generation_args: {\n",
       "              max_new_tokens: 65,\n",
       "          },\n",
       "          constrained_decoding: False,\n",
       "      },\n",
       "  })]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3210e-03, -2.7548e-02, -1.7815e-02,  ...,  3.0987e-03,\n",
       "         -3.0957e-08, -2.0683e-04],\n",
       "        [-7.8455e-02,  3.5944e-02, -3.9946e-02,  ...,  3.3455e-05,\n",
       "         -2.8308e-07, -6.4833e-04],\n",
       "        [ 2.0934e-03,  4.0179e-03, -7.0349e-03,  ..., -1.0699e-04,\n",
       "          1.5696e-09,  4.7038e-05],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "          3.5755e-07, -3.2188e-04],\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "                 nan,  2.9316e-03],\n",
       "        [        nan,         nan,         nan,  ...,         nan,\n",
       "                 nan,         nan]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"Anthony_Jeselnik\"][1].sequence_attributions[0].target_attributions.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ada/humor/data/input_gradient.json\"\n",
    "torch.save(results, file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
