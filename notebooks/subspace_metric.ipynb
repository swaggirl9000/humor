{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM humor detection with Subspace based metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ada/humor/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    output_hidden_states=True  # Enable hidden states output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv('/home/ada/humor/data/stand_up_dataset/standup_data.csv')\n",
    "transcript = pd.read_csv('/home/ada/humor/data/stand_up_dataset/standup_transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = [\n",
    "    \"Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"The following is a stand-up comedy transcript. When performed in front of a live audience, which jokes do you think made the audience laugh?  List of quotes:\",\n",
    "    \"You are a person who enjoys aggressive humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys self-enhancing humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys self-deprecating humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys dark humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys affiliative humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"The following is a stand-up comedy transcript. What are the funniest punchlines from the transcript. List of quotes:\",\n",
    "    \"Below is a transcript from a stand-up comedy routine. Analyze the transcript and extract the quotes that are most likely to have made the audience laugh. List of quotes:\",\n",
    "    \"The following is a stand-up comedy transcript. When preformed in front of a live audience, which jokes do you think made the audience laugh? List of quotes:\",\n",
    "    \"Pretend that you are a stand-up comedian reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"Pretend that you are a stand-up comedy fan reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"Pretend that you are a stand-up comedy critic reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\" \n",
    "]\n",
    "\n",
    "TRANSCRIPTS = [\n",
    "    {\"comedian\": row[\"comedian\"], \"transcript\": row[\"transcript\"]}\n",
    "    for index, row in transcript.head(20).iterrows()\n",
    "]\n",
    "\n",
    "transcript_comedians = {comedian[\"comedian\"] for comedian in TRANSCRIPTS} \n",
    "GROUND_TRUTHS = []\n",
    "for index, row in ground_truth.iterrows():\n",
    "    if row[\"comedian\"] not in transcript_comedians: continue\n",
    "    found_comedian = False\n",
    "    for comedian_dict in GROUND_TRUTHS:\n",
    "        if comedian_dict[\"comedian\"] == row[\"comedian\"]:\n",
    "            comedian_dict[\"sentence\"].append(row[\"sentence\"])\n",
    "            found_comedian = True\n",
    "            break\n",
    "    \n",
    "    if not found_comedian:\n",
    "        GROUND_TRUTHS.append({\n",
    "            \"comedian\": row[\"comedian\"],\n",
    "            \"sentence\": [row[\"sentence\"]]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 125.12 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 10\u001b[0m    outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m representation \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomedian\u001b[39m\u001b[38;5;124m\"\u001b[39m: text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomedian\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrep\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcat(outputs\u001b[38;5;241m.\u001b[39mhidden_states)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m:]\u001b[38;5;241m.\u001b[39mflatten()}\n\u001b[1;32m     13\u001b[0m all_representations\u001b[38;5;241m.\u001b[39mappend(representation)\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/transformers/models/gemma2/modeling_gemma2.py:1088\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(logits)\n\u001b[1;32m   1086\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfinal_logit_softcapping\n\u001b[0;32m-> 1088\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 125.12 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_representations = []\n",
    "for inst, text in product(INSTRUCTIONS, TRANSCRIPTS):\n",
    "   comedian_name = text['comedian']\n",
    "   matching_truths = [truth['sentence'] for truth in GROUND_TRUTHS if truth['comedian'] == comedian_name]\n",
    "   sentence = f\"{inst}\\n{text['transcript']}\\n\" + \"\\n - \".join(matching_truths[0])\n",
    "   inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "   with torch.inference_mode():\n",
    "      outputs = model(**inputs)\n",
    "\n",
    "   representation = {\"comedian\": text[\"comedian\"], \"rep\": torch.cat(outputs.hidden_states)[-1, -16:].flatten()}\n",
    "   all_representations.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# all_representations = []\n",
    "# for inst, text in product(INSTRUCTIONS, TRANSCRIPTS):\n",
    "#    sentence = f\"{inst}\\n{text}\\n\" + \"\\n -\".join(GROUND_TRUTHS)\n",
    "#    inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "#    with torch.inference_mode():\n",
    "#        outputs = model(**inputs)\n",
    "\n",
    "#   representation = torch.cat(outputs.hidden_states)[-1, -16:].flatten()\n",
    "#    all_representations.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57344, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_reference_list = []\n",
    "for comedian, rep in all_representations:\n",
    "    gt_references = torch.stack(rep)\n",
    "    *_, gt_reference_subspace = torch.pca_lowrank(gt_references.float(), q=10)\n",
    "    gt_reference_list.append({\"comedian\": comedian, \"subspace\": gt_reference_subspace})\n",
    "    \n",
    "# gt_reference_subspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [f\"{inst}\\n{text}\\n\" for inst, text in product(INSTRUCTIONS, TRANSCRIPTS)]\n",
    "\n",
    "#inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "#with torch.inference_mode():\n",
    "    #outputs = model(input_ids=model.generate(**inputs, max_new_tokens=128))\n",
    "\n",
    "\n",
    "# representations = torch.cat(outputs.hidden_states)[-1, -16:].flatten()\n",
    "# all_representations.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_representations = []\n",
    "for inst, text in product(INSTRUCTIONS, TRANSCRIPTS):\n",
    "    prompt = f\"{inst}\\n{text['transcript']}\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids=model.generate(**inputs, max_new_tokens=128))\n",
    "    representation = {\"comedian\": text[\"comedian\"], \"rep\": torch.cat(outputs.hidden_states)[-1, -16:].flatten()}\n",
    "    all_representations.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reference_answers = []\n",
    "for comedian, rep in all_representations:\n",
    "    model_references = torch.stack(rep)\n",
    "    *_, model_reference_subspace = torch.pca_lowrank(model_references.float(), q=10)\n",
    "    model_reference_answers.append({\"comedian\": comedian, \"subspace\": model_reference_subspace})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "\n",
    "for model_entry in model_reference_answers:\n",
    "    comedian = model_entry[\"comedian\"]\n",
    "    subspace = model_entry[\"subspace\"]\n",
    "    gt_entry = next(entry for entry in gt_reference_list if entry[\"comedian\"] == comedian)\n",
    "    gt_subspace = gt_entry[\"subspace\"] \n",
    "        \n",
    "    A = subspace @ gt_subspace\n",
    "    score = A @ A / A.shape[0]\n",
    "    metric.append({\"comedian\": comedian, \"score\": score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3584])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1][-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
