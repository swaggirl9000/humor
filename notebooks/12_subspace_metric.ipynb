{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM humor detection with Subspace based metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/humor/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487a6fbe4ab0415ea774148e4a0f96c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    output_hidden_states=True  # Enable hidden states output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv('../data/stand_up_dataset/standup_data.csv')\n",
    "transcript = pd.read_csv('../data/stand_up_dataset/standup_transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = [\n",
    "    \"Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"The following is a stand-up comedy transcript. When performed in front of a live audience, which jokes do you think made the audience laugh?  List of quotes:\",\n",
    "    \"You are a person who enjoys aggressive humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys self-enhancing humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys self-deprecating humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys dark humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys affiliative humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"The following is a stand-up comedy transcript. What are the funniest punchlines from the transcript. List of quotes:\",\n",
    "    # \"Below is a transcript from a stand-up comedy routine. Analyze the transcript and extract the quotes that are most likely to have made the audience laugh. List of quotes:\",\n",
    "    # \"The following is a stand-up comedy transcript. When preformed in front of a live audience, which jokes do you think made the audience laugh? List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedian reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedy fan reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedy critic reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    #\"Analyze the stand-up comedy transcript below. Which lines and punchlines do you think delivered the biggest laughs to the audience? List of quotes:\", \n",
    "    #\"As a person who enjoys witty, intellectual humor, extract the key humorous lines and punchlines from this stand-up comedy transcript. Focus on the quotes that demonstrate clever wordplay or insights. List of quotes:\",\n",
    "    #\"This is a transcript from a stand-up routine. Identify the lines and punchlines that likely had the strongest comedic impact during the performance. List of quotes:\",\n",
    "    #\"Pretend you're an audience member at this stand-up show. Which lines do you think got the biggest laughs? Focus on key moments of humor. List of quotes:\",\n",
    "    #\"This is a transcript of a live stand-up performance. Which quotes do you believe would have resonated the most with the audience? Focus on key punchlines. List of quotes:\",\n",
    "    #\"Imagine you are a comedian reviewing this stand-up routine. Identify the funniest moments and lines where the punchlines landed the hardest. List of quotes:\",\n",
    "    #\"Read through the stand-up comedy transcript and extract the lines that best capture the humor and timing of the performance. Focus on punchlines that likely had the audience laughing. List of quotes:\",\n",
    "    #\"This is a stand-up comedy transcript. Analyze the content and extract the lines that most effectively build up to or deliver punchlines. List of quotes:\",\n",
    "    #\"Pretend you're watching this performance live. What do you think were the standout comedic lines and punchlines that elicited the loudest laughs? List of quotes:\",\n",
    "    #\"Imagine you are writing a review of this stand-up performance. What lines and punchlines would you highlight as the funniest moments? List of quotes:\"\n",
    "]\n",
    "\n",
    "CONTENTS = [\n",
    "    \"\",\n",
    "    \"Sure, here are the key humorous lines:\",\n",
    "    \"Here are some lines and punchlines that could be funny:\",\n",
    "    \"Got it! Here are the main punchlines and comedic highlights:\",\n",
    "    # \"Here's a selection of the funniest quotes from the transcript:\",\n",
    "    # \"I've picked out the key humorous moments for you:\",\n",
    "    # \"Below are the standout lines and punchlines from the performance:\",\n",
    "    # \"Here's a breakdown of the top quotes that likely got the biggest laughs:\",\n",
    "    # \"Take a look at these key comedic lines from the routine:\",\n",
    "    # \"Here's a list of the most memorable punchlines from the set:\",\n",
    "    # \"Check out these quotes—some of the best comedic moments from the transcript:\",\n",
    "    # \"Here are the funniest moments and punchlines I found in the transcript:\",\n",
    "    # \"Here's what I've identified as the standout lines and punchlines in this comedy routine:\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>instruction</th>\n",
       "      <th>content</th>\n",
       "      <th>gt_input</th>\n",
       "      <th>model_input</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comedian</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Sure, here are the key humorous lines:</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Here are some lines and punchlines that could ...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>The following is a stand-up comedy transcript....</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nThe following is a s...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nThe following is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys aggressive humor. ...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Sure, here are the key humorous lines:</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Here are some lines and punchlines that could ...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         transcript  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "\n",
       "                                                       ground_truth  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "\n",
       "                                                        instruction  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  The following is a stand-up comedy transcript....   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      You are a person who enjoys aggressive humor. ...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "\n",
       "                                                            content  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik                                                      \n",
       "Anthony_Jeselnik             Sure, here are the key humorous lines:   \n",
       "Anthony_Jeselnik  Here are some lines and punchlines that could ...   \n",
       "Anthony_Jeselnik  Got it! Here are the main punchlines and comed...   \n",
       "Anthony_Jeselnik                                                      \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      Got it! Here are the main punchlines and comed...   \n",
       "Tom_Segura_3                                                          \n",
       "Tom_Segura_3                 Sure, here are the key humorous lines:   \n",
       "Tom_Segura_3      Here are some lines and punchlines that could ...   \n",
       "Tom_Segura_3      Got it! Here are the main punchlines and comed...   \n",
       "\n",
       "                                                           gt_input  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nThe following is a s...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "\n",
       "                                                        model_input  \n",
       "comedian                                                             \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nThe following is a s...  \n",
       "...                                                             ...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "\n",
       "[816 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = ground_truth.groupby(\"comedian\")[\"sentence\"].apply(list).apply(lambda sentences: \"\\n\".join([f\"{i + 1}. {s}\" for i, s in enumerate(sentences)]))\n",
    "df = transcript.set_index(\"comedian\").join(gt).rename(columns={\"sentence\": \"ground_truth\"})\n",
    "\n",
    "df[\"instruction\"] = [INSTRUCTIONS] * len(df)\n",
    "df = df.explode(\"instruction\")\n",
    "df[\"content\"] = [CONTENTS] * len(df)\n",
    "df = df.explode(\"content\")\n",
    "\n",
    "def gt_chat_template(row):\n",
    "    return tokenizer.apply_chat_template([\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"] + \"\\n\" + row[\"transcript\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"content\"] + \"\\n\" + row[\"ground_truth\"]},\n",
    "    ], tokenize=False)\n",
    "\n",
    "df[\"gt_input\"] = df.apply(gt_chat_template, axis=1)\n",
    "\n",
    "def model_chat_template(row):\n",
    "    return tokenizer.apply_chat_template([\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"] + \"\\n\" + row[\"transcript\"]},\n",
    "    ], tokenize=False)\n",
    "\n",
    "df[\"model_input\"] = df.apply(model_chat_template, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(df.iloc[:2, -1].tolist(), return_tensors=\"pt\", padding=True, truncation=False).to(model.device)\n",
    "\n",
    "x = model.generate(**inputs, max_new_tokens=4)\n",
    "\n",
    "# matrix_repr = model.lm_head.weight.float()[ids]\n",
    "\n",
    "# *_, subs_repr = torch.pca_lowrank(matrix_repr, q=8)\n",
    "\n",
    "# subs_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = model.lm_head.weight.float().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 79.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# use unembedding tokenization form\n",
    "def get_gt_representation(batch_of_strs: list[str], subspace_size: int = 8) -> torch.Tensor:\n",
    "    inputs = tokenizer(batch_of_strs, return_tensors=\"pt\", padding=True, truncation=False).to(model.device)\n",
    "    *_, subs_repr = torch.pca_lowrank(U[inputs[\"input_ids\"]], q=subspace_size)\n",
    "    return subs_repr\n",
    "\n",
    "gt_representations = {\n",
    "    comedian: get_gt_representation(batch.tolist())\n",
    "    for comedian, batch in tqdm(df.groupby(\"comedian\")[\"model_input\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2304, 8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_representations[\"Ali_Wong\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [03:39<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_output_representation(batch_of_strs: list[str], number_of_tokens: int = 128, subspace_size: int = 8) -> torch.Tensor:\n",
    "    inputs = tokenizer(batch_of_strs, return_tensors=\"pt\", padding=True, truncation=False).to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        ids = model.generate(**inputs, max_new_tokens=number_of_tokens)\n",
    "    \n",
    "    *_, subs_repr = torch.pca_lowrank(U[ids], q=subspace_size)\n",
    "    return subs_repr\n",
    "        \n",
    "\n",
    "output_representations = {\n",
    "    comedian: get_output_representation(batch.tolist())\n",
    "    for comedian, batch in tqdm(df.groupby(\"comedian\")[\"gt_input\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2304, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_representations[\"Ali_Wong\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2985, device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gt_representations[\"Ali_Wong\"].mT @ output_representations[\"Ali_Wong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2304, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_representations[\"Ali_Wong\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 2842.31it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for comedian in tqdm(gt_representations.keys()):\n",
    "    gt_reference_subspaces = gt_representations[comedian]\n",
    "    out_reference_subspaces = output_representations[comedian]\n",
    "\n",
    "    A = gt_reference_subspaces.mT @ out_reference_subspaces\n",
    "    scores[comedian] = A.matrix_power(2).diagonal(dim1=1,dim2=2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comedian</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali_Wong</td>\n",
       "      <td>0.298467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ali_Wong_2</td>\n",
       "      <td>0.388907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ali_Wong_3</td>\n",
       "      <td>0.381492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthony_Jeselnik</td>\n",
       "      <td>0.430651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthony_Jeselnik_2</td>\n",
       "      <td>0.398048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anthony_Jeselnik_3</td>\n",
       "      <td>0.405424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chelsea_Peretti</td>\n",
       "      <td>0.391905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chelsea_Peretti_2</td>\n",
       "      <td>0.339364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chelsea_Peretti_3</td>\n",
       "      <td>0.292384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Donald_Glover</td>\n",
       "      <td>0.345824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Donald_Glover_2</td>\n",
       "      <td>0.373793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Donald_Glover_3</td>\n",
       "      <td>0.421870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hasan_Minhaj</td>\n",
       "      <td>0.316128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hasan_Minhaj_2</td>\n",
       "      <td>0.365989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hasan_Minhaj_3</td>\n",
       "      <td>0.393355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iliza_Shlesinger</td>\n",
       "      <td>0.536874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iliza_Shlesinger_2</td>\n",
       "      <td>0.434934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iliza_Shlesinger_3</td>\n",
       "      <td>0.328990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jim_Gaffigan</td>\n",
       "      <td>0.322485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jim_Gaffigan_2</td>\n",
       "      <td>0.288111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jim_Gaffigan_3</td>\n",
       "      <td>0.283017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jimmy_Yang</td>\n",
       "      <td>0.353025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Jimmy_Yang_2</td>\n",
       "      <td>0.391994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jimmy_Yang_3</td>\n",
       "      <td>0.325063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Joe_List</td>\n",
       "      <td>0.323564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Joe_List_2</td>\n",
       "      <td>0.385618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Joe_List_3</td>\n",
       "      <td>0.337625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>John_Mulaney</td>\n",
       "      <td>0.400418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>John_Mulaney_2</td>\n",
       "      <td>0.345999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>John_Mulaney_3</td>\n",
       "      <td>0.439053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Louis_CK</td>\n",
       "      <td>0.323249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Louis_CK_2</td>\n",
       "      <td>0.353228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Louis_CK_3</td>\n",
       "      <td>0.494558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Nate_Bargatze</td>\n",
       "      <td>0.329367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nate_Bargatze_2</td>\n",
       "      <td>0.336355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nate_Bargatze_3</td>\n",
       "      <td>0.416929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Nate_Bargatze_TK</td>\n",
       "      <td>0.332358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Nate_Bargatze_TK_2</td>\n",
       "      <td>0.319535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Nate_Bargatze_TK_3</td>\n",
       "      <td>0.403290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Russell_Peters</td>\n",
       "      <td>0.339276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Russell_Peters_2</td>\n",
       "      <td>0.374948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Russell_Peters_3</td>\n",
       "      <td>0.356161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Sam_Morill_3</td>\n",
       "      <td>0.407484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sam_Morril</td>\n",
       "      <td>0.326882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sam_Morril_2</td>\n",
       "      <td>0.353726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Tom_Segura</td>\n",
       "      <td>0.366718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Tom_Segura_2</td>\n",
       "      <td>0.310674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Tom_Segura_3</td>\n",
       "      <td>0.395768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Trevor_Noah</td>\n",
       "      <td>0.550599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Trevor_Noah_2</td>\n",
       "      <td>0.370816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Trevor_Noah_3</td>\n",
       "      <td>0.431686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comedian     Score\n",
       "0             Ali_Wong  0.298467\n",
       "1           Ali_Wong_2  0.388907\n",
       "2           Ali_Wong_3  0.381492\n",
       "3     Anthony_Jeselnik  0.430651\n",
       "4   Anthony_Jeselnik_2  0.398048\n",
       "5   Anthony_Jeselnik_3  0.405424\n",
       "6      Chelsea_Peretti  0.391905\n",
       "7    Chelsea_Peretti_2  0.339364\n",
       "8    Chelsea_Peretti_3  0.292384\n",
       "9        Donald_Glover  0.345824\n",
       "10     Donald_Glover_2  0.373793\n",
       "11     Donald_Glover_3  0.421870\n",
       "12        Hasan_Minhaj  0.316128\n",
       "13      Hasan_Minhaj_2  0.365989\n",
       "14      Hasan_Minhaj_3  0.393355\n",
       "15    Iliza_Shlesinger  0.536874\n",
       "16  Iliza_Shlesinger_2  0.434934\n",
       "17  Iliza_Shlesinger_3  0.328990\n",
       "18        Jim_Gaffigan  0.322485\n",
       "19      Jim_Gaffigan_2  0.288111\n",
       "20      Jim_Gaffigan_3  0.283017\n",
       "21          Jimmy_Yang  0.353025\n",
       "22        Jimmy_Yang_2  0.391994\n",
       "23        Jimmy_Yang_3  0.325063\n",
       "24            Joe_List  0.323564\n",
       "25          Joe_List_2  0.385618\n",
       "26          Joe_List_3  0.337625\n",
       "27        John_Mulaney  0.400418\n",
       "28      John_Mulaney_2  0.345999\n",
       "29      John_Mulaney_3  0.439053\n",
       "30            Louis_CK  0.323249\n",
       "31          Louis_CK_2  0.353228\n",
       "32          Louis_CK_3  0.494558\n",
       "33       Nate_Bargatze  0.329367\n",
       "34     Nate_Bargatze_2  0.336355\n",
       "35     Nate_Bargatze_3  0.416929\n",
       "36    Nate_Bargatze_TK  0.332358\n",
       "37  Nate_Bargatze_TK_2  0.319535\n",
       "38  Nate_Bargatze_TK_3  0.403290\n",
       "39      Russell_Peters  0.339276\n",
       "40    Russell_Peters_2  0.374948\n",
       "41    Russell_Peters_3  0.356161\n",
       "42        Sam_Morill_3  0.407484\n",
       "43          Sam_Morril  0.326882\n",
       "44        Sam_Morril_2  0.353726\n",
       "45          Tom_Segura  0.366718\n",
       "46        Tom_Segura_2  0.310674\n",
       "47        Tom_Segura_3  0.395768\n",
       "48         Trevor_Noah  0.550599\n",
       "49       Trevor_Noah_2  0.370816\n",
       "50       Trevor_Noah_3  0.431686"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(scores, index=range(len(scores))).to_csv(\"scores.csv\")\n",
    "df = pd.DataFrame(list(scores.items()), columns=['Comedian', 'Score'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
