{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM humor detection with Subspace based metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ada/humor/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from more_itertools import batched\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(next(cwd.glob(\"**/standup_data.csv\")))\n",
    "transcript = pd.read_csv(next(cwd.glob(\"**/standup_transcripts.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = [\n",
    "    \"Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"The following is a stand-up comedy transcript. When performed in front of a live audience, which jokes do you think made the audience laugh?  List of quotes:\",\n",
    "    \"You are a person who enjoys aggressive humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    \"You are a person who enjoys self-enhancing humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys self-deprecating humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys dark humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"You are a person who enjoys affiliative humor. Extract the key humorous lines and punchlines for this stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"The following is a stand-up comedy transcript. What are the funniest punchlines from the transcript. List of quotes:\",\n",
    "    # \"Below is a transcript from a stand-up comedy routine. Analyze the transcript and extract the quotes that are most likely to have made the audience laugh. List of quotes:\",\n",
    "    # \"The following is a stand-up comedy transcript. When preformed in front of a live audience, which jokes do you think made the audience laugh? List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedian reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedy fan reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    # \"Pretend that you are a stand-up comedy critic reading the following stand-up comedy transcript. Focus on the quotes highlighting the main comedic moments. List of quotes:\",\n",
    "    #\"Analyze the stand-up comedy transcript below. Which lines and punchlines do you think delivered the biggest laughs to the audience? List of quotes:\", \n",
    "    #\"As a person who enjoys witty, intellectual humor, extract the key humorous lines and punchlines from this stand-up comedy transcript. Focus on the quotes that demonstrate clever wordplay or insights. List of quotes:\",\n",
    "    #\"This is a transcript from a stand-up routine. Identify the lines and punchlines that likely had the strongest comedic impact during the performance. List of quotes:\",\n",
    "    #\"Pretend you're an audience member at this stand-up show. Which lines do you think got the biggest laughs? Focus on key moments of humor. List of quotes:\",\n",
    "    #\"This is a transcript of a live stand-up performance. Which quotes do you believe would have resonated the most with the audience? Focus on key punchlines. List of quotes:\",\n",
    "    #\"Imagine you are a comedian reviewing this stand-up routine. Identify the funniest moments and lines where the punchlines landed the hardest. List of quotes:\",\n",
    "    #\"Read through the stand-up comedy transcript and extract the lines that best capture the humor and timing of the performance. Focus on punchlines that likely had the audience laughing. List of quotes:\",\n",
    "    #\"This is a stand-up comedy transcript. Analyze the content and extract the lines that most effectively build up to or deliver punchlines. List of quotes:\",\n",
    "    #\"Pretend you're watching this performance live. What do you think were the standout comedic lines and punchlines that elicited the loudest laughs? List of quotes:\",\n",
    "    #\"Imagine you are writing a review of this stand-up performance. What lines and punchlines would you highlight as the funniest moments? List of quotes:\"\n",
    "]\n",
    "\n",
    "CONTENTS = [\n",
    "    \"\",\n",
    "    \"Sure, here are the key humorous lines:\",\n",
    "    \"Here are some lines and punchlines that could be funny:\",\n",
    "    \"Got it! Here are the main punchlines and comedic highlights:\",\n",
    "    # \"Here's a selection of the funniest quotes from the transcript:\",\n",
    "    # \"I've picked out the key humorous moments for you:\",\n",
    "    # \"Below are the standout lines and punchlines from the performance:\",\n",
    "    # \"Here's a breakdown of the top quotes that likely got the biggest laughs:\",\n",
    "    # \"Take a look at these key comedic lines from the routine:\",\n",
    "    # \"Here's a list of the most memorable punchlines from the set:\",\n",
    "    # \"Check out these quotes—some of the best comedic moments from the transcript:\",\n",
    "    # \"Here are the funniest moments and punchlines I found in the transcript:\",\n",
    "    # \"Here's what I've identified as the standout lines and punchlines in this comedy routine:\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>instruction</th>\n",
       "      <th>content</th>\n",
       "      <th>gt_input</th>\n",
       "      <th>model_input</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comedian</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Sure, here are the key humorous lines:</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Here are some lines and punchlines that could ...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>Extract the key humorous lines and punchlines ...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nExtract the key humo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthony_Jeselnik</th>\n",
       "      <td>When I was a kid, I used to fantasize about ge...</td>\n",
       "      <td>1. So poor I remember, just so I could go to m...</td>\n",
       "      <td>The following is a stand-up comedy transcript....</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nThe following is a s...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nThe following is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys aggressive humor. ...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Sure, here are the key humorous lines:</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Here are some lines and punchlines that could ...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom_Segura_3</th>\n",
       "      <td>Probably checked in to 400 hotels this year. A...</td>\n",
       "      <td>1. And the guy goes, “Whoa. Are you Japanese?”...</td>\n",
       "      <td>You are a person who enjoys self-enhancing hum...</td>\n",
       "      <td>Got it! Here are the main punchlines and comed...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "      <td>&lt;bos&gt;&lt;start_of_turn&gt;user\\nYou are a person who...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         transcript  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "Anthony_Jeselnik  When I was a kid, I used to fantasize about ge...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "Tom_Segura_3      Probably checked in to 400 hotels this year. A...   \n",
       "\n",
       "                                                       ground_truth  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "Anthony_Jeselnik  1. So poor I remember, just so I could go to m...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "Tom_Segura_3      1. And the guy goes, “Whoa. Are you Japanese?”...   \n",
       "\n",
       "                                                        instruction  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  Extract the key humorous lines and punchlines ...   \n",
       "Anthony_Jeselnik  The following is a stand-up comedy transcript....   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      You are a person who enjoys aggressive humor. ...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "Tom_Segura_3      You are a person who enjoys self-enhancing hum...   \n",
       "\n",
       "                                                            content  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik                                                      \n",
       "Anthony_Jeselnik             Sure, here are the key humorous lines:   \n",
       "Anthony_Jeselnik  Here are some lines and punchlines that could ...   \n",
       "Anthony_Jeselnik  Got it! Here are the main punchlines and comed...   \n",
       "Anthony_Jeselnik                                                      \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      Got it! Here are the main punchlines and comed...   \n",
       "Tom_Segura_3                                                          \n",
       "Tom_Segura_3                 Sure, here are the key humorous lines:   \n",
       "Tom_Segura_3      Here are some lines and punchlines that could ...   \n",
       "Tom_Segura_3      Got it! Here are the main punchlines and comed...   \n",
       "\n",
       "                                                           gt_input  \\\n",
       "comedian                                                              \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...   \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nThe following is a s...   \n",
       "...                                                             ...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...   \n",
       "\n",
       "                                                        model_input  \n",
       "comedian                                                             \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nExtract the key humo...  \n",
       "Anthony_Jeselnik  <bos><start_of_turn>user\\nThe following is a s...  \n",
       "...                                                             ...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "Tom_Segura_3      <bos><start_of_turn>user\\nYou are a person who...  \n",
       "\n",
       "[816 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = ground_truth.groupby(\"comedian\")[\"sentence\"].apply(list).apply(lambda sentences: \"\\n\".join([f\"{i + 1}. {s}\" for i, s in enumerate(sentences)]))\n",
    "df = transcript.set_index(\"comedian\").join(gt).rename(columns={\"sentence\": \"ground_truth\"})\n",
    "\n",
    "df[\"instruction\"] = [INSTRUCTIONS] * len(df)\n",
    "df = df.explode(\"instruction\")\n",
    "df[\"content\"] = [CONTENTS] * len(df)\n",
    "df = df.explode(\"content\")\n",
    "\n",
    "def gt_chat_template(row):\n",
    "    return tokenizer.apply_chat_template([\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"] + \"\\n\" + row[\"transcript\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"content\"] + \"\\n\" + row[\"ground_truth\"]},\n",
    "    ], tokenize=False)\n",
    "\n",
    "df[\"gt_input\"] = df.apply(gt_chat_template, axis=1)\n",
    "\n",
    "def model_chat_template(row):\n",
    "    return tokenizer.apply_chat_template([\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"] + \"\\n\" + row[\"transcript\"]},\n",
    "    ], tokenize=False)\n",
    "\n",
    "df[\"model_input\"] = df.apply(model_chat_template, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = model.lm_head.weight.float().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 51.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# use unembedding tokenization form\n",
    "def get_gt_representation(batch_of_strs: list, subspace_size: int = 8) -> torch.Tensor:\n",
    "    inputs = tokenizer(batch_of_strs, return_tensors=\"pt\", padding=True, truncation=False).to(model.device)\n",
    "    *_, subs_repr = torch.pca_lowrank(U[inputs[\"input_ids\"]], q=subspace_size)\n",
    "    return subs_repr\n",
    "\n",
    "gt_representations = {\n",
    "    comedian: get_gt_representation(batch.tolist())\n",
    "    for comedian, batch in tqdm(df.groupby(\"comedian\")[\"model_input\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2048, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_representations[\"Ali_Wong\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/51 [00:13<01:04,  1.50s/it]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.10 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 20.90 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 8.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subs_repr\n\u001b[1;32m     11\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[0;32m---> 13\u001b[0m output_representations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     comedian: torch\u001b[38;5;241m.\u001b[39mcat([get_output_representation(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched(batch\u001b[38;5;241m.\u001b[39mtolist(), BATCH_SIZE)])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comedian, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_input\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m }\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subs_repr\n\u001b[1;32m     11\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     13\u001b[0m output_representations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 14\u001b[0m     comedian: torch\u001b[38;5;241m.\u001b[39mcat([get_output_representation(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched(batch\u001b[38;5;241m.\u001b[39mtolist(), BATCH_SIZE)])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comedian, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_input\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m }\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subs_repr\n\u001b[1;32m     11\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     13\u001b[0m output_representations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 14\u001b[0m     comedian: torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mget_output_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched(batch\u001b[38;5;241m.\u001b[39mtolist(), BATCH_SIZE)])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comedian, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_input\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m }\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mget_output_representation\u001b[0;34m(batch_of_strs, number_of_tokens, subspace_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_of_strs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 5\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;241m*\u001b[39m_, subs_repr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpca_lowrank(U[ids], q\u001b[38;5;241m=\u001b[39msubspace_size)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subs_repr\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/humor/.venv/lib/python3.8/site-packages/transformers/models/gemma/modeling_gemma.py:1142\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1140\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1141\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1142\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.10 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 20.90 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 8.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def get_output_representation(batch_of_strs: list, number_of_tokens: int = 128, subspace_size: int = 8) -> torch.Tensor:\n",
    "    inputs = tokenizer(batch_of_strs, return_tensors=\"pt\", padding=True, truncation=False).to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        ids = model.generate(**inputs, max_new_tokens=number_of_tokens)\n",
    "    \n",
    "    *_, subs_repr = torch.pca_lowrank(U[ids], q=subspace_size)\n",
    "    return subs_repr\n",
    "        \n",
    "\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "output_representations = {\n",
    "    comedian: torch.cat([get_output_representation(x) for x in batched(batch.tolist(), BATCH_SIZE)])\n",
    "    for comedian, batch in tqdm(df.groupby(\"comedian\")[\"gt_input\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for comedian in tqdm(gt_representations.keys()):\n",
    "    gt_reference_subspaces = gt_representations[comedian]\n",
    "    out_reference_subspaces = output_representations[comedian]\n",
    "\n",
    "    A = gt_reference_subspaces.mT @ out_reference_subspaces\n",
    "    scores[comedian] = A.matrix_power(2).diagonal(dim1=1,dim2=2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(scores, index=range(len(scores))).to_csv(\"scores.csv\")\n",
    "df = pd.DataFrame(list(scores.items()), columns=['Comedian', 'Score'])\n",
    "df[\"Score\"].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = next(cwd.glob(\"**/subspace_scores/*.csv\")).parent / f\"scores_{MODEL_ID.rsplit('/')[-1]}.csv\"\n",
    "df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")\n",
    "from scipy.stats import spearmanr\n",
    "from humor.bipartite_metric import bipartite_metric\n",
    "\n",
    "ground_truth = pd.read_csv('../data/stand_up_dataset/standup_data.csv')\n",
    "model = pd.read_csv('../data/stand_up_dataset/gemma_answers.csv')\n",
    "\n",
    "gemma_metric = bipartite_metric(model, ground_truth)\n",
    "merged_df = pd.merge(gemma_metric, df, on='comedian', suffixes=('_df1', '_df2'))\n",
    "correlation, p_value = spearmanr(merged_df['score_df1'], merged_df['score_df2'])\n",
    "print(\"Correlation: \", correlation)\n",
    "print(\"p_value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
